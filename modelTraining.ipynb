{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Java Development Kit (OpenJDK 11 is commonly compatible with Spark 3.x)\n",
        "!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# 2. Set JAVA_HOME environment variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# 3. Add JAVA_HOME to PATH\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "print(\"Java 11 installed and JAVA_HOME set.\")\n",
        "print(f\"JAVA_HOME: {os.environ.get('JAVA_HOME')}\")\n",
        "\n",
        "# 4. Download Spark (Updated link for Spark 3.5.1 with Hadoop 3)\n",
        "#    Ensure this version is compatible with your Java (e.g., Spark 3.x with Hadoop 3.x for Java 11)\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# 5. Unzip the file\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# 6. Set SPARK_HOME environment variable\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "# 7. Add SPARK_HOME/bin to PATH\n",
        "os.environ[\"PATH\"] = os.environ[\"SPARK_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "print(\"Spark downloaded, unzipped, and SPARK_HOME set.\")\n",
        "print(f\"SPARK_HOME: {os.environ.get('SPARK_HOME')}\")\n",
        "\n",
        "# Verify installations (optional)\n",
        "!java -version\n",
        "!spark-submit --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlbrP9Ok67AP",
        "outputId": "456cc620-f0c2-4aa6-f54e-2f08874050ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jre-headless_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 91.189.92.23 80]\n",
            "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/o/openjdk-lts/openjdk-11-jdk-headless_11.0.28%2b6-1ubuntu1%7e22.04.1_amd64.deb  404  Not Found [IP: 91.189.92.23 80]\n",
            "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
            "Java 11 installed and JAVA_HOME set.\n",
            "JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64\n",
            "--2025-12-08 00:48:50--  https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400446614 (382M) [application/x-gzip]\n",
            "Saving to: ‚Äòspark-3.5.1-bin-hadoop3.tgz‚Äô\n",
            "\n",
            "spark-3.5.1-bin-had 100%[===================>] 381.90M  12.6MB/s    in 34s     \n",
            "\n",
            "2025-12-08 00:49:25 (11.4 MB/s) - ‚Äòspark-3.5.1-bin-hadoop3.tgz‚Äô saved [400446614/400446614]\n",
            "\n",
            "Spark downloaded, unzipped, and SPARK_HOME set.\n",
            "SPARK_HOME: /content/spark-3.5.1-bin-hadoop3\n",
            "openjdk version \"17.0.16\" 2025-07-15\n",
            "OpenJDK Runtime Environment (build 17.0.16+8-Ubuntu-0ubuntu122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.16+8-Ubuntu-0ubuntu122.04.1, mixed mode, sharing)\n",
            "/content/spark-3.5.1-bin-hadoop3/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
            "/content/spark-3.5.1-bin-hadoop3/bin/spark-class: line 97: CMD: bad array subscript\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwnujxikIOy3",
        "outputId": "66b946e7-627f-473a-9aee-7b814cbd519a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "  inflating: US_Accidents_March23.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qubqJUT287Wu",
        "outputId": "a998de3e-9c5c-440c-b335-340c54c3901a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è  Ignoring kernel file argument: /root/.local/share/jupyter/runtime/kernel-a662ef09-b24c-4f2e-a0ec-273d4ae7db98.json\n",
            "\n",
            "======================================================================\n",
            "   TRAFFIC ACCIDENT SEVERITY PREDICTION SYSTEM\n",
            "   Using Apache Spark MLlib for Big Data Processing\n",
            "======================================================================\n",
            "\n",
            "‚úì Auto-detected CSV file: /content/US_Accidents_March23.csv\n",
            "\n",
            "‚úì Using CSV file: /content/US_Accidents_March23.csv\n",
            "\n",
            "üöÄ Initializing Spark Session...\n",
            "‚úì Spark Session created successfully\n",
            "   Spark Version: 3.5.1\n",
            "============================================================\n",
            "STEP 1: DATA INGESTION\n",
            "============================================================\n",
            "Loading data from CSV file: /content/US_Accidents_March23.csv\n",
            "‚úì Loaded 7,728,394 records with 46 columns\n",
            "\n",
            "üìã Column names (first 10): ['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)']\n",
            "   ... and 36 more columns\n",
            "‚úì Load time: 11.37 seconds\n",
            "\n",
            "============================================================\n",
            "STEP 2: DATA EXPLORATION\n",
            "============================================================\n",
            "\n",
            "üìä Dataset Schema (first 20 columns):\n",
            "   - ID: StringType()\n",
            "   - Source: StringType()\n",
            "   - Severity: IntegerType()\n",
            "   - Start_Time: TimestampType()\n",
            "   - End_Time: TimestampType()\n",
            "   - Start_Lat: DoubleType()\n",
            "   - Start_Lng: DoubleType()\n",
            "   - End_Lat: DoubleType()\n",
            "   - End_Lng: DoubleType()\n",
            "   - Distance(mi): DoubleType()\n",
            "   - Description: StringType()\n",
            "   - Street: StringType()\n",
            "   - City: StringType()\n",
            "   - County: StringType()\n",
            "   - State: StringType()\n",
            "   - Zipcode: StringType()\n",
            "   - Country: StringType()\n",
            "   - Timezone: StringType()\n",
            "   - Airport_Code: StringType()\n",
            "   - Weather_Timestamp: TimestampType()\n",
            "\n",
            "   ... and 26 more columns\n",
            "\n",
            "üìà Severity Distribution (Target Variable) - using column 'Severity':\n",
            "+--------+-------+\n",
            "|Severity|  count|\n",
            "+--------+-------+\n",
            "|       1|  67366|\n",
            "|       2|6156981|\n",
            "|       3|1299337|\n",
            "|       4| 204710|\n",
            "+--------+-------+\n",
            "\n",
            "Class Imbalance Analysis:\n",
            "   Severity 1: 67,366 records (0.87%) Kishan\n",
            "   Severity 2: 6,156,981 records (79.67%) Kishan\n",
            "   Severity 3: 1,299,337 records (16.81%) Kishan\n",
            "   Severity 4: 204,710 records (2.65%) Kishan\n",
            "\n",
            "============================================================\n",
            "STEP 3: DATA CLEANING (ETL)\n",
            "============================================================\n",
            "‚úì Selected 14 relevant columns\n",
            "‚úì Dropped 0 rows with missing critical values\n",
            "‚úì Final clean dataset: 7,728,394 records\n",
            "‚úì Data retention rate: 100.00%\n",
            "\n",
            "============================================================\n",
            "STEP 4: FEATURE ENGINEERING (ENHANCED)\n",
            "============================================================\n",
            "‚úì Extracted Hour, DayOfWeek, and Month from timestamp\n",
            "‚úì Created IsRushHour feature\n",
            "‚úì Created IsWeekend feature\n",
            "‚úì Created TimeOfDay feature\n",
            "‚úì Created Season feature\n",
            "‚úì Converted boolean features to numeric\n",
            "‚úì Created Temp_Humidity_Interaction feature\n",
            "‚úì Created Wind_Visibility_Interaction feature\n",
            "\n",
            "üìã Sample of engineered features:\n",
            "+----+---------+----------+---------+---------+------+\n",
            "|Hour|DayOfWeek|IsRushHour|IsWeekend|TimeOfDay|Season|\n",
            "+----+---------+----------+---------+---------+------+\n",
            "|   5|        2|       0.0|      0.0|      4.0|   4.0|\n",
            "|   6|        2|       0.0|      0.0|      1.0|   4.0|\n",
            "|   6|        2|       0.0|      0.0|      1.0|   4.0|\n",
            "|   7|        2|       1.0|      0.0|      1.0|   4.0|\n",
            "|   7|        2|       1.0|      0.0|      1.0|   4.0|\n",
            "+----+---------+----------+---------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "============================================================\n",
            "STEP 5: HANDLING CLASS IMBALANCE (IMPROVED)\n",
            "============================================================\n",
            "\n",
            "Class Weights (Balanced - Improved):\n",
            "Note: Severity is 0-based (0, 1, 2, 3) corresponding to original (1, 2, 3, 4)\n",
            "   Severity 1 (index 0):       67,366 samples ( 0.87%) -> weight = 28.6806\n",
            "   Severity 2 (index 1):    6,156,981 samples (79.67%) -> weight = 0.3138\n",
            "   Severity 4 (index 3):      204,710 samples ( 2.65%) -> weight = 9.4382\n",
            "   Severity 3 (index 2):    1,299,337 samples (16.81%) -> weight = 1.4870\n",
            "\n",
            "‚úì Added classWeight column to handle imbalance\n",
            "  Using balanced class weights for better minority class performance\n",
            "\n",
            "============================================================\n",
            "STEP 6: BUILDING ML PIPELINE\n",
            "============================================================\n",
            "‚úì Added Weather_Condition encoder\n",
            "‚úì Added Sunrise_Sunset encoder\n",
            "‚úì Feature columns: 21 features\n",
            "‚úì Added VectorAssembler\n",
            "‚úì Added RandomForestClassifier (optimized for accuracy)\n",
            "  Parameters: 100 trees, maxDepth=12, maxBins=32, featureSubsetStrategy='sqrt'\n",
            "  Supports multiclass classification (4 severity levels: 0, 1, 2, 3)\n",
            "\n",
            "============================================================\n",
            "STEP 7: MODEL TRAINING (IMPROVED)\n",
            "============================================================\n",
            "‚úì Training set: 6,181,605 records\n",
            "‚úì Test set: 1,546,789 records\n",
            "\n",
            "üîÑ Training model... (this may take several minutes)\n",
            "\n",
            "‚ùå Error during execution: An error occurred while calling o1433.fit.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 100.0 failed 1 times, most recent failure: Lost task 2.0 in stage 100.0 (TID 5544) (88403a01cf8d executor driver): java.lang.OutOfMemoryError: Java heap space\n",
            "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5640/0x00007881053b2588.apply(Unknown Source)\n",
            "\tat scala.Array$.tabulate(Array.scala:418)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5632/0x00007881053a7c00.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD$$Lambda$2963/0x0000788104e4b5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2718/0x0000788104dbb5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
            "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
            "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
            "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
            "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
            "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5640/0x00007881053b2588.apply(Unknown Source)\n",
            "\tat scala.Array$.tabulate(Array.scala:418)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5632/0x00007881053a7c00.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD$$Lambda$2963/0x0000788104e4b5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2718/0x0000788104dbb5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\t... 1 more\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1564900877.py\", line 850, in main\n",
            "    model, predictions, test_data = train_and_evaluate(df_weighted, pipeline, use_cross_validation=False)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1564900877.py\", line 570, in train_and_evaluate\n",
            "    model = pipeline.fit(train_data)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/ml/base.py\", line 205, in fit\n",
            "    return self._fit(dataset)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
            "    model = stage.fit(dataset)\n",
            "            ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/ml/base.py\", line 205, in fit\n",
            "    return self._fit(dataset)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
            "    java_model = self._fit_java(dataset)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
            "    return self._java_obj.fit(dataset._jdf)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o1433.fit.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 100.0 failed 1 times, most recent failure: Lost task 2.0 in stage 100.0 (TID 5544) (88403a01cf8d executor driver): java.lang.OutOfMemoryError: Java heap space\n",
            "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5640/0x00007881053b2588.apply(Unknown Source)\n",
            "\tat scala.Array$.tabulate(Array.scala:418)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5632/0x00007881053a7c00.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD$$Lambda$2963/0x0000788104e4b5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2718/0x0000788104dbb5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:738)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:737)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n",
            "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
            "\tat scala.util.Try$.apply(Try.scala:213)\n",
            "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
            "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\n",
            "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\n",
            "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
            "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
            "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
            "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22(RandomForest.scala:651)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$22$adapted(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5640/0x00007881053b2588.apply(Unknown Source)\n",
            "\tat scala.Array$.tabulate(Array.scala:418)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$.$anonfun$findBestSplits$21(RandomForest.scala:647)\n",
            "\tat org.apache.spark.ml.tree.impl.RandomForest$$$Lambda$5632/0x00007881053a7c00.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
            "\tat org.apache.spark.rdd.RDD$$Lambda$2963/0x0000788104e4b5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
            "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
            "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
            "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
            "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
            "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2718/0x0000788104dbb5a0.apply(Unknown Source)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
            "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
            "\t... 1 more\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úì Spark Session stopped\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, hour, dayofweek, when, lit, create_map, count\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from itertools import chain\n",
        "import time\n",
        "import pandas as pd\n",
        "import argparse\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Initialize Spark Session with optimized configurations for large datasets.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Stop any existing Spark session (common issue in Colab/Jupyter)\n",
        "        try:\n",
        "            existing_spark = SparkSession.getActiveSession()\n",
        "            if existing_spark is not None:\n",
        "                print(\"‚ö†Ô∏è  Stopping existing Spark session...\")\n",
        "                existing_spark.stop()\n",
        "                # Give it a moment to fully stop\n",
        "                import time\n",
        "                time.sleep(1)\n",
        "        except:\n",
        "            pass  # No existing session or error stopping it\n",
        "\n",
        "        # Try creating Spark session with full configuration\n",
        "        try:\n",
        "            spark = SparkSession.builder \\\n",
        "                .appName(\"AccidentSeverityPrediction\") \\\n",
        "                .config(\"spark.driver.memory\", \"12g\") \\\n",
        "                .config(\"spark.executor.memory\", \"12g\") \\\n",
        "                .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
        "                .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "                .config(\"spark.default.parallelism\", \"100\") \\\n",
        "                .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "                .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "                .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=200\") \\\n",
        "                .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=200\") \\\n",
        "                .master(\"local[*]\") \\\n",
        "                .getOrCreate()\n",
        "        except Exception as e1:\n",
        "            print(f\"‚ö†Ô∏è  Failed with full config, trying simpler configuration...\")\n",
        "            # Fallback: Try with minimal configuration (common in Colab)\n",
        "            try:\n",
        "                spark = SparkSession.builder \\\n",
        "                    .appName(\"AccidentSeverityPrediction\") \\\n",
        "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .getOrCreate()\n",
        "                print(\"‚úì Created Spark session with minimal configuration\")\n",
        "            except Exception as e2:\n",
        "                # Last resort: Try with absolute minimum\n",
        "                print(f\"‚ö†Ô∏è  Failed with minimal config, trying absolute minimum...\")\n",
        "                spark = SparkSession.builder \\\n",
        "                    .appName(\"AccidentSeverityPrediction\") \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .getOrCreate()\n",
        "                print(\"‚úì Created Spark session with minimum configuration\")\n",
        "\n",
        "        # Validate that Spark session was created successfully\n",
        "        if spark is None:\n",
        "            raise RuntimeError(\"Failed to create Spark session - got None\")\n",
        "\n",
        "        # Validate sparkContext exists\n",
        "        if not hasattr(spark, 'sparkContext') or spark.sparkContext is None:\n",
        "            raise RuntimeError(\"Spark session created but sparkContext is None\")\n",
        "\n",
        "        # Set log level - use INFO to see stage progress, WARN to reduce verbosity\n",
        "        # INFO shows stage progress bars which are helpful for monitoring training\n",
        "        # In Colab, you'll see progress like: [Stage 95:==============================> (56 + 8) / 100]\n",
        "        spark.sparkContext.setLogLevel(\"INFO\")\n",
        "\n",
        "        return spark\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error creating Spark session: {e}\")\n",
        "        print(\"\\nTroubleshooting tips:\")\n",
        "        print(\"  1. Ensure PySpark is installed: !pip install pyspark\")\n",
        "        print(\"  2. Ensure Java is installed and JAVA_HOME is correctly set in your environment (e.g., in Colab: `!apt-get install -y openjdk-11-jdk-headless` and `os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'`).\")\n",
        "        print(\"  3. A Colab runtime restart might be needed after installing/setting Java.\")\n",
        "        raise\n",
        "\n",
        "def load_data(spark, filepath, sample_fraction=None):\n",
        "    \"\"\"Load CSV data into Spark DataFrame with schema inference.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 1: DATA INGESTION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not filepath:\n",
        "        raise ValueError(\"CSV file path is required!\")\n",
        "\n",
        "    import os\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {filepath}\")\n",
        "\n",
        "    # Validate file extension\n",
        "    file_ext = os.path.splitext(filepath)[1].lower()\n",
        "    if file_ext not in ['.csv', '']:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: File extension is '{file_ext}', expected '.csv'\")\n",
        "        print(\"   The file might not be a CSV file. Continuing anyway...\")\n",
        "\n",
        "    # Check if file looks like a Jupyter kernel file (common mistake)\n",
        "    if 'kernel-' in filepath and filepath.endswith('.json'):\n",
        "        raise ValueError(\n",
        "            f\"\\n‚ùå ERROR: The file path appears to be a Jupyter kernel file, not a CSV file!\\n\"\n",
        "            f\"   File: {filepath}\\n\"\n",
        "            f\"   This looks like a Jupyter runtime file, not your data file.\\n\"\n",
        "            f\"   Please provide the path to your CSV file (e.g., 'US_Accidents_March23.csv')\\n\"\n",
        "            f\"   Example usage: python accident_severity_prediction.py US_Accidents_March23.csv\"\n",
        "        )\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Loading data from CSV file: {filepath}\")\n",
        "    df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
        "\n",
        "    record_count = df.count()\n",
        "    column_count = len(df.columns)\n",
        "    column_names = df.columns\n",
        "\n",
        "    print(f\"‚úì Loaded {record_count:,} records with {column_count} columns\")\n",
        "\n",
        "    # Validate that this looks like a real CSV file\n",
        "    if column_count == 1 and column_names[0] in ['{', '_c0', 'value']:\n",
        "        raise ValueError(\n",
        "            f\"\\n‚ùå ERROR: The file doesn't appear to be a valid CSV file!\\n\"\n",
        "            f\"   File: {filepath}\\n\"\n",
        "            f\"   Found only 1 column: {column_names[0]}\\n\"\n",
        "            f\"   This might be a JSON file or incorrectly formatted CSV.\\n\"\n",
        "            f\"   Please check:\\n\"\n",
        "            f\"   1. The file is actually a CSV file\\n\"\n",
        "            f\"   2. The file has headers in the first row\\n\"\n",
        "            f\"   3. You're providing the correct file path\\n\"\n",
        "            f\"   Example: python accident_severity_prediction.py US_Accidents_March23.csv\"\n",
        "        )\n",
        "\n",
        "    # Show first few column names for verification\n",
        "    print(f\"\\nüìã Column names (first 10): {column_names[:10]}\")\n",
        "    if column_count > 10:\n",
        "        print(f\"   ... and {column_count - 10} more columns\")\n",
        "\n",
        "    # Check if Severity column exists (case-insensitive)\n",
        "    severity_found = any(col.lower() == \"severity\" for col in column_names)\n",
        "    if not severity_found:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: No 'Severity' column found (case-insensitive check)\")\n",
        "        print(\"   The script will attempt to continue, but may fail if Severity column is missing.\")\n",
        "\n",
        "    # Apply sampling if requested\n",
        "    if sample_fraction is not None and 0 < sample_fraction < 1:\n",
        "        print(f\"\\nüìä Sampling {sample_fraction*100:.1f}% of data for faster training...\")\n",
        "        df = df.sample(fraction=sample_fraction, seed=42)\n",
        "        sampled_count = df.count()\n",
        "        print(f\"‚úì Sampled dataset: {sampled_count:,} records ({sample_fraction*100:.1f}% of original)\")\n",
        "\n",
        "    print(f\"‚úì Load time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def find_column_case_insensitive(df, column_name):\n",
        "    \"\"\"Find a column in DataFrame case-insensitively.\"\"\"\n",
        "    all_columns = [field.name for field in df.schema.fields]\n",
        "    for col_name in all_columns:\n",
        "        if col_name.lower() == column_name.lower():\n",
        "            return col_name\n",
        "    return None\n",
        "\n",
        "def explore_data(df):\n",
        "    \"\"\"Perform initial data exploration.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 2: DATA EXPLORATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Show schema\n",
        "    print(\"\\nüìä Dataset Schema (first 20 columns):\")\n",
        "    for field in df.schema.fields[:20]:\n",
        "        print(f\"   - {field.name}: {field.dataType}\")\n",
        "\n",
        "    # Show all columns if there are more than 20\n",
        "    if len(df.schema.fields) > 20:\n",
        "        print(f\"\\n   ... and {len(df.schema.fields) - 20} more columns\")\n",
        "\n",
        "    # Check if Severity column exists (case-insensitive)\n",
        "    all_columns = [field.name for field in df.schema.fields]\n",
        "    severity_col = find_column_case_insensitive(df, \"Severity\")\n",
        "\n",
        "    if severity_col is None:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: 'Severity' column not found in the dataset!\")\n",
        "        print(\"\\nüìã Available columns in the dataset:\")\n",
        "        for i, col_name in enumerate(all_columns, 1):\n",
        "            print(f\"   {i}. {col_name}\")\n",
        "        print(\"\\n‚ùå Error: The dataset must contain a 'Severity' column (case-insensitive).\")\n",
        "        print(\"   Please check your CSV file and ensure it has a 'Severity' column.\")\n",
        "        raise ValueError(f\"'Severity' column not found. Available columns: {all_columns[:10]}...\")\n",
        "\n",
        "    # Show severity distribution\n",
        "    print(f\"\\nüìà Severity Distribution (Target Variable) - using column '{severity_col}':\")\n",
        "    severity_dist = df.groupBy(severity_col).count().orderBy(severity_col)\n",
        "    severity_dist.show()\n",
        "\n",
        "    # Calculate class imbalance\n",
        "    total = df.count()\n",
        "    severity_counts = severity_dist.collect()\n",
        "    print(\"Class Imbalance Analysis:\")\n",
        "    for row in severity_counts:\n",
        "        pct = (row['count'] / total) * 100\n",
        "        print(f\"   Severity {row[severity_col]}: {row['count']:,} records ({pct:.2f}%) Kishan\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean and preprocess the data - ETL Pipeline.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 3: DATA CLEANING (ETL)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    initial_count = df.count()\n",
        "\n",
        "    # Find columns case-insensitively\n",
        "    column_mapping = {}\n",
        "    desired_columns = [\n",
        "        \"Severity\", \"Start_Lat\", \"Start_Lng\", \"Temperature(F)\",\n",
        "        \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\",\n",
        "        \"Weather_Condition\", \"Sunrise_Sunset\", \"Start_Time\",\n",
        "        \"Crossing\", \"Junction\", \"Traffic_Signal\"\n",
        "    ]\n",
        "\n",
        "    for desired_col in desired_columns:\n",
        "        found_col = find_column_case_insensitive(df, desired_col)\n",
        "        if found_col:\n",
        "            column_mapping[desired_col] = found_col\n",
        "\n",
        "    # Check for critical columns\n",
        "    if \"Severity\" not in column_mapping:\n",
        "        all_columns = [field.name for field in df.schema.fields]\n",
        "        print(f\"\\n‚ùå Error: 'Severity' column not found!\")\n",
        "        print(f\"Available columns: {all_columns}\")\n",
        "        raise ValueError(\"'Severity' column is required but not found in the dataset\")\n",
        "\n",
        "    # Select columns using their actual names (case-insensitive match)\n",
        "    existing_columns = list(column_mapping.values())\n",
        "    df_clean = df.select(existing_columns)\n",
        "\n",
        "    # Rename columns to standard names for easier processing\n",
        "    for desired_col, actual_col in column_mapping.items():\n",
        "        if actual_col != desired_col:\n",
        "            df_clean = df_clean.withColumnRenamed(actual_col, desired_col)\n",
        "\n",
        "    print(f\"‚úì Selected {len(existing_columns)} relevant columns\")\n",
        "\n",
        "    # Drop rows with null values in critical columns\n",
        "    critical_columns = [\"Severity\", \"Start_Lat\", \"Start_Lng\", \"Start_Time\"]\n",
        "    critical_columns = [c for c in critical_columns if c in df_clean.columns]\n",
        "    df_clean = df_clean.dropna(subset=critical_columns)\n",
        "\n",
        "    after_critical_drop = df_clean.count()\n",
        "    print(f\"‚úì Dropped {initial_count - after_critical_drop:,} rows with missing critical values\")\n",
        "\n",
        "    # Filter out invalid severity values (keep only 1-4)\n",
        "    df_clean = df_clean.filter((col(\"Severity\") >= 1) & (col(\"Severity\") <= 4))\n",
        "\n",
        "    # Convert severity to 0-based indexing (0, 1, 2, 3) for better ML compatibility\n",
        "    # Original: 1, 2, 3, 4 -> New: 0, 1, 2, 3\n",
        "    df_clean = df_clean.withColumn(\"Severity\", col(\"Severity\") - 1.0)\n",
        "\n",
        "    # Cast severity to double for ML\n",
        "    df_clean = df_clean.withColumn(\"Severity\", col(\"Severity\").cast(DoubleType()))\n",
        "\n",
        "    # Handle missing values in numerical columns with median imputation\n",
        "    numerical_cols = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\"]\n",
        "    numerical_cols = [c for c in numerical_cols if c in df_clean.columns]\n",
        "\n",
        "    if numerical_cols:\n",
        "        # Fill nulls with column means for simplicity (Imputer requires no nulls in output)\n",
        "        for col_name in numerical_cols:\n",
        "            mean_val = df_clean.select(col_name).agg({col_name: \"mean\"}).collect()[0][0]\n",
        "            if mean_val is not None:\n",
        "                df_clean = df_clean.fillna({col_name: mean_val})\n",
        "\n",
        "    # Handle missing categorical values\n",
        "    if \"Weather_Condition\" in df_clean.columns:\n",
        "        df_clean = df_clean.fillna({\"Weather_Condition\": \"Unknown\"})\n",
        "    if \"Sunrise_Sunset\" in df_clean.columns:\n",
        "        df_clean = df_clean.fillna({\"Sunrise_Sunset\": \"Day\"})\n",
        "\n",
        "    final_count = df_clean.count()\n",
        "    print(f\"‚úì Final clean dataset: {final_count:,} records\")\n",
        "    print(f\"‚úì Data retention rate: {(final_count/initial_count)*100:.2f}%\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def engineer_features(df):\n",
        "    \"\"\"Extract and engineer features from the cleaned data - Enhanced version.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 4: FEATURE ENGINEERING (ENHANCED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Extract temporal features from Start_Time\n",
        "    df = df.withColumn(\"Hour\", hour(col(\"Start_Time\")))\n",
        "    df = df.withColumn(\"DayOfWeek\", dayofweek(col(\"Start_Time\")))\n",
        "    df = df.withColumn(\"Month\", col(\"Start_Time\").substr(6, 2).cast(\"int\"))\n",
        "\n",
        "    print(\"‚úì Extracted Hour, DayOfWeek, and Month from timestamp\")\n",
        "\n",
        "    # Create rush hour indicator (7-9 AM and 4-7 PM)\n",
        "    df = df.withColumn(\"IsRushHour\",\n",
        "        when(((col(\"Hour\") >= 7) & (col(\"Hour\") <= 9)) |\n",
        "             ((col(\"Hour\") >= 16) & (col(\"Hour\") <= 19)), 1.0).otherwise(0.0))\n",
        "\n",
        "    print(\"‚úì Created IsRushHour feature\")\n",
        "\n",
        "    # Create weekend indicator\n",
        "    df = df.withColumn(\"IsWeekend\",\n",
        "        when((col(\"DayOfWeek\") == 1) | (col(\"DayOfWeek\") == 7), 1.0).otherwise(0.0))\n",
        "\n",
        "    print(\"‚úì Created IsWeekend feature\")\n",
        "\n",
        "    # Create time of day categories (Morning, Afternoon, Evening, Night)\n",
        "    df = df.withColumn(\"TimeOfDay\",\n",
        "        when((col(\"Hour\") >= 6) & (col(\"Hour\") < 12), 1.0)  # Morning\n",
        "        .when((col(\"Hour\") >= 12) & (col(\"Hour\") < 17), 2.0)  # Afternoon\n",
        "        .when((col(\"Hour\") >= 17) & (col(\"Hour\") < 22), 3.0)  # Evening\n",
        "        .otherwise(4.0))  # Night\n",
        "\n",
        "    print(\"‚úì Created TimeOfDay feature\")\n",
        "\n",
        "    # Create season feature (if Month available)\n",
        "    if \"Month\" in df.columns:\n",
        "        df = df.withColumn(\"Season\",\n",
        "            when((col(\"Month\") >= 3) & (col(\"Month\") <= 5), 1.0)  # Spring\n",
        "            .when((col(\"Month\") >= 6) & (col(\"Month\") <= 8), 2.0)  # Summer\n",
        "            .when((col(\"Month\") >= 9) & (col(\"Month\") <= 11), 3.0)  # Fall\n",
        "            .otherwise(4.0))  # Winter\n",
        "        print(\"‚úì Created Season feature\")\n",
        "\n",
        "    # Convert boolean columns to numeric\n",
        "    boolean_cols = [\"Crossing\", \"Junction\", \"Traffic_Signal\"]\n",
        "    for col_name in boolean_cols:\n",
        "        if col_name in df.columns:\n",
        "            df = df.withColumn(col_name,\n",
        "                when(col(col_name) == True, 1.0)\n",
        "                .when(col(col_name) == False, 0.0)\n",
        "                .otherwise(0.0))\n",
        "\n",
        "    print(\"‚úì Converted boolean features to numeric\")\n",
        "\n",
        "    # Create interaction features (important for XGBoost-style models)\n",
        "    if \"Temperature(F)\" in df.columns and \"Humidity(%)\" in df.columns:\n",
        "        df = df.withColumn(\"Temp_Humidity_Interaction\",\n",
        "            col(\"Temperature(F)\") * col(\"Humidity(%)\") / 100.0)\n",
        "        print(\"‚úì Created Temp_Humidity_Interaction feature\")\n",
        "\n",
        "    if \"Wind_Speed(mph)\" in df.columns and \"Visibility(mi)\" in df.columns:\n",
        "        df = df.withColumn(\"Wind_Visibility_Interaction\",\n",
        "            col(\"Wind_Speed(mph)\") / (col(\"Visibility(mi)\") + 0.1))  # Avoid division by zero\n",
        "        print(\"‚úì Created Wind_Visibility_Interaction feature\")\n",
        "\n",
        "    # Drop the original Start_Time column (no longer needed)\n",
        "    df = df.drop(\"Start_Time\")\n",
        "\n",
        "    # Show sample of engineered features\n",
        "    print(\"\\nüìã Sample of engineered features:\")\n",
        "    sample_cols = [\"Hour\", \"DayOfWeek\", \"IsRushHour\", \"IsWeekend\", \"TimeOfDay\"]\n",
        "    if \"Season\" in df.columns:\n",
        "        sample_cols.append(\"Season\")\n",
        "    df.select(sample_cols).show(5)\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_class_weights(df):\n",
        "    \"\"\"Calculate class weights to handle imbalanced data - Improved method.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 5: HANDLING CLASS IMBALANCE (IMPROVED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Calculate class distribution\n",
        "    class_counts = df.groupBy(\"Severity\").count().collect()\n",
        "\n",
        "    total_samples = sum([row[\"count\"] for row in class_counts])\n",
        "    num_classes = len(class_counts)\n",
        "\n",
        "    # Improved weight calculation: using sklearn's balanced method\n",
        "    # weight = n_samples / (n_classes * np.bincount(y))\n",
        "    # This gives more balanced weights\n",
        "    class_weights = {}\n",
        "    print(\"\\nClass Weights (Balanced - Improved):\")\n",
        "    print(\"Note: Severity is 0-based (0, 1, 2, 3) corresponding to original (1, 2, 3, 4)\")\n",
        "    for row in class_counts:\n",
        "        severity = row[\"Severity\"]\n",
        "        count = row[\"count\"]\n",
        "        # Balanced weight formula (similar to sklearn)\n",
        "        weight = total_samples / (num_classes * count)\n",
        "        class_weights[severity] = weight\n",
        "        pct = (count / total_samples) * 100\n",
        "        original_severity = int(severity) + 1  # Convert back to 1-4 for display\n",
        "        print(f\"   Severity {original_severity} (index {int(severity)}): {count:>12,} samples ({pct:>5.2f}%) -> weight = {weight:.4f}\")\n",
        "\n",
        "    # Create mapping expression for adding weights\n",
        "    mapping_expr = create_map([lit(x) for x in chain(*class_weights.items())])\n",
        "\n",
        "    # Add weight column to dataset\n",
        "    df_weighted = df.withColumn(\"classWeight\", mapping_expr[col(\"Severity\")])\n",
        "\n",
        "    print(\"\\n‚úì Added classWeight column to handle imbalance\")\n",
        "    print(\"  Using balanced class weights for better minority class performance\")\n",
        "\n",
        "    return df_weighted\n",
        "\n",
        "def build_ml_pipeline(df):\n",
        "    \"\"\"Build the ML pipeline with feature transformers and classifier.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 6: BUILDING ML PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    stages = []\n",
        "\n",
        "    # String Indexer for Weather_Condition\n",
        "    if \"Weather_Condition\" in df.columns:\n",
        "        weather_indexer = StringIndexer(\n",
        "            inputCol=\"Weather_Condition\",\n",
        "            outputCol=\"Weather_Index\",\n",
        "            handleInvalid=\"skip\"\n",
        "        )\n",
        "        weather_encoder = OneHotEncoder(\n",
        "            inputCols=[\"Weather_Index\"],\n",
        "            outputCols=[\"Weather_Vec\"]\n",
        "        )\n",
        "        stages.extend([weather_indexer, weather_encoder])\n",
        "        print(\"‚úì Added Weather_Condition encoder\")\n",
        "\n",
        "    # String Indexer for Sunrise_Sunset\n",
        "    if \"Sunrise_Sunset\" in df.columns:\n",
        "        sunrise_indexer = StringIndexer(\n",
        "            inputCol=\"Sunrise_Sunset\",\n",
        "            outputCol=\"Sunrise_Index\",\n",
        "            handleInvalid=\"skip\"\n",
        "        )\n",
        "        sunrise_encoder = OneHotEncoder(\n",
        "            inputCols=[\"Sunrise_Index\"],\n",
        "            outputCols=[\"Sunrise_Vec\"]\n",
        "        )\n",
        "        stages.extend([sunrise_indexer, sunrise_encoder])\n",
        "        print(\"‚úì Added Sunrise_Sunset encoder\")\n",
        "\n",
        "    # Define feature columns for the assembler\n",
        "    numerical_features = [\n",
        "        \"Start_Lat\", \"Start_Lng\", \"Hour\", \"DayOfWeek\",\n",
        "        \"IsRushHour\", \"IsWeekend\"\n",
        "    ]\n",
        "\n",
        "    # Add enhanced features if they exist\n",
        "    enhanced_features = [\"TimeOfDay\", \"Month\", \"Season\"]\n",
        "    for col_name in enhanced_features:\n",
        "        if col_name in df.columns:\n",
        "            numerical_features.append(col_name)\n",
        "\n",
        "    # Add optional numerical features if they exist\n",
        "    optional_numerical = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\",\n",
        "                          \"Visibility(mi)\", \"Wind_Speed(mph)\",\n",
        "                          \"Crossing\", \"Junction\", \"Traffic_Signal\",\n",
        "                          \"Temp_Humidity_Interaction\", \"Wind_Visibility_Interaction\"]\n",
        "\n",
        "    for col_name in optional_numerical:\n",
        "        if col_name in df.columns:\n",
        "            numerical_features.append(col_name)\n",
        "\n",
        "    # Build final feature list\n",
        "    feature_cols = numerical_features.copy()\n",
        "    if \"Weather_Condition\" in df.columns:\n",
        "        feature_cols.append(\"Weather_Vec\")\n",
        "    if \"Sunrise_Sunset\" in df.columns:\n",
        "        feature_cols.append(\"Sunrise_Vec\")\n",
        "\n",
        "    print(f\"‚úì Feature columns: {len(feature_cols)} features\")\n",
        "\n",
        "    # Vector Assembler\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=feature_cols,\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "    stages.append(assembler)\n",
        "    print(\"‚úì Added VectorAssembler\")\n",
        "\n",
        "    # Use Random Forest for multiclass classification (4 severity levels: 0, 1, 2, 3)\n",
        "    # Random Forest supports multiclass natively and works well with class weights\n",
        "    # Optimized parameters based on Kaggle best practices for better accuracy\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        labelCol=\"Severity\",\n",
        "        featuresCol=\"features\",\n",
        "        weightCol=\"classWeight\",\n",
        "        numTrees=200,  # Reduced for memory efficiency\n",
        "        maxDepth=20,    # Reduced for memory efficiency\n",
        "        minInstancesPerNode=10,  # Minimum samples per node\n",
        "        seed=42,\n",
        "        subsamplingRate=0.8,  # Row sampling (80% of data per tree)\n",
        "        featureSubsetStrategy=\"sqrt\",  # Better feature sampling (sqrt of features)\n",
        "        maxBins=32,  # Increased for better feature handling\n",
        "        impurity=\"gini\",  # Gini impurity for classification\n",
        "        bootstrap=True  # Bootstrap sampling\n",
        "    )\n",
        "    stages.append(rf)\n",
        "    print(\"‚úì Added RandomForestClassifier (optimized for accuracy)\")\n",
        "    print(\"  Parameters: 100 trees, maxDepth=12, maxBins=32, featureSubsetStrategy='sqrt'\")\n",
        "    print(\"  Supports multiclass classification (4 severity levels: 0, 1, 2, 3)\")\n",
        "\n",
        "    # Build pipeline\n",
        "    pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "def train_and_evaluate(df, pipeline, use_cross_validation=False):\n",
        "    \"\"\"Train the model and evaluate performance with improved techniques.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 7: MODEL TRAINING (IMPROVED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Split data: 80% train, 20% test\n",
        "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    train_count = train_data.count()\n",
        "    test_count = test_data.count()\n",
        "\n",
        "    print(f\"‚úì Training set: {train_count:,} records\")\n",
        "    print(f\"‚úì Test set: {test_count:,} records\")\n",
        "\n",
        "    # Cache training data for faster access\n",
        "    train_data.cache()\n",
        "\n",
        "    if use_cross_validation:\n",
        "        # Cross-validation for hyperparameter tuning\n",
        "        print(\"\\nüîÑ Performing Cross-Validation for hyperparameter tuning...\")\n",
        "        evaluator = MulticlassClassificationEvaluator(\n",
        "            labelCol=\"Severity\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"f1\"\n",
        "        )\n",
        "\n",
        "        # Parameter grid for tuning\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(pipeline.stages[-1].maxDepth, [10, 15, 20]) \\\n",
        "            .addGrid(pipeline.stages[-1].maxIter if hasattr(pipeline.stages[-1], 'maxIter')\\\n",
        "                    else pipeline.stages[-1].numTrees, [100, 150, 200]) \\\n",
        "            .build()\n",
        "\n",
        "        cv = CrossValidator(\n",
        "            estimator=pipeline,\n",
        "            estimatorParamMaps=paramGrid,\n",
        "            evaluator=evaluator,\n",
        "            numFolds=3,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        cv_model = cv.fit(train_data)\n",
        "        training_time = time.time() - start_time\n",
        "        model = cv_model.bestModel\n",
        "        print(f\"‚úì Cross-validation completed in {training_time:.2f} seconds\")\n",
        "        print(f\"‚úì Best parameters selected\")\n",
        "    else:\n",
        "        # Direct training (faster)\n",
        "        print(\"\\nüîÑ Training model... (this may take several minutes)\")\n",
        "        start_time = time.time()\n",
        "        model = pipeline.fit(train_data)\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"‚úì Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"\\nüîÑ Making predictions on test set...\")\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # Unpersist cached data\n",
        "    train_data.unpersist()\n",
        "\n",
        "    return model, predictions, test_data\n",
        "\n",
        "def evaluate_model(predictions):\n",
        "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 8: MODEL EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"accuracy\"\n",
        "    )\n",
        "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Weighted Precision\n",
        "    precision_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"weightedPrecision\"\n",
        "    )\n",
        "    precision = precision_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Weighted Recall\n",
        "    recall_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"weightedRecall\"\n",
        "    )\n",
        "    recall = recall_evaluator.evaluate(predictions)\n",
        "\n",
        "    # F1 Score (most important for imbalanced data)\n",
        "    f1_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"f1\"\n",
        "    )\n",
        "    f1_score = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "    print(\"\\nüìä MODEL PERFORMANCE METRICS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"   Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"   Weighted Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"   Weighted Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "    print(f\"   F1 Score:           {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Per-class metrics\n",
        "    print(\"\\nüìã Confusion Matrix Analysis:\")\n",
        "    predictions.groupBy(\"Severity\", \"prediction\").count().orderBy(\"Severity\", \"prediction\").show(20)\n",
        "\n",
        "    # Prediction distribution\n",
        "    print(\"\\nüìà Prediction Distribution:\")\n",
        "    predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "\n",
        "def extract_feature_importance(model, feature_names):\n",
        "    \"\"\"Extract and display feature importance from the trained model.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 9: FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Get the model from the pipeline (could be GBT or RF)\n",
        "        ml_model = model.stages[-1]\n",
        "\n",
        "        # Get feature importances\n",
        "        if hasattr(ml_model, 'featureImportances'):\n",
        "            importances = ml_model.featureImportances.toArray()\n",
        "        else:\n",
        "            print(\"‚ö† Model doesn't support feature importance extraction\")\n",
        "            return None\n",
        "\n",
        "        # Create feature importance pairs\n",
        "        feature_importance = list(zip(feature_names, importances))\n",
        "\n",
        "        # Sort by importance (descending)\n",
        "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(\"\\nüîù Top 20 Most Important Features:\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, (feature, importance) in enumerate(feature_importance[:20], 1):\n",
        "            bar = \"‚ñà\" * int(importance * 100)\n",
        "            print(f\"{i:2}. {feature:30} {importance:.4f} {bar}\")\n",
        "\n",
        "        return feature_importance\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Could not extract feature importance: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def save_model(model, path=\"accident_severity_model\"):\n",
        "    \"\"\"Save the trained model to disk.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 10: SAVING MODEL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        model.write().overwrite().save(path)\n",
        "        print(f\"‚úì Model saved to: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Could not save model: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    import sys\n",
        "    import os\n",
        "\n",
        "    # Filter out IPython/Colab magic command flags (like -f) and kernel files before parsing\n",
        "    # These flags are used by %run magic command and shouldn't be passed to the script\n",
        "    filtered_argv = [sys.argv[0]]  # Keep script name\n",
        "    for arg in sys.argv[1:]:\n",
        "        # Skip magic command flags\n",
        "        if arg in ['-f', '-i', '-e', '-t', '-N', '-n', '-p']:\n",
        "            continue\n",
        "        # Skip kernel JSON files (common mistake in Colab)\n",
        "        if 'kernel-' in arg and arg.endswith('.json'):\n",
        "            print(f\"‚ö†Ô∏è  Ignoring kernel file argument: {arg}\")\n",
        "            continue\n",
        "        # Skip if it looks like a kernel file path\n",
        "        if '/jupyter/runtime/kernel-' in arg:\n",
        "            print(f\"‚ö†Ô∏è  Ignoring kernel file path: {arg}\")\n",
        "            continue\n",
        "        filtered_argv.append(arg)\n",
        "\n",
        "    original_argv = sys.argv\n",
        "    sys.argv = filtered_argv\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(description='Traffic Accident Severity Prediction using Apache Spark MLlib')\n",
        "    parser.add_argument('csv_file', nargs='?', help='Path to the CSV file containing accident data')\n",
        "    parser.add_argument('--sample', type=float, default=None,\n",
        "                       help='Sample fraction (0.0 to 1.0) to use for training. Example: 0.1 for 10%%')\n",
        "\n",
        "    # Use parse_known_args to ignore any remaining unknown arguments (Colab compatibility)\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    # Restore original argv\n",
        "    sys.argv = original_argv\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"   TRAFFIC ACCIDENT SEVERITY PREDICTION SYSTEM\")\n",
        "    print(\"   Using Apache Spark MLlib for Big Data Processing\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Get CSV file path from command line argument or prompt\n",
        "    if args.csv_file:\n",
        "        csv_filepath = args.csv_file\n",
        "    else:\n",
        "        # Try to auto-detect CSV file in common Colab locations\n",
        "        common_csv_names = [\n",
        "            'US_Accidents_March23.csv',\n",
        "            'US_Accidents.csv',\n",
        "            'accidents.csv',\n",
        "            'data.csv'\n",
        "        ]\n",
        "        common_paths = [\n",
        "            '/content/',\n",
        "            './',\n",
        "            ''\n",
        "        ]\n",
        "\n",
        "        csv_filepath = None\n",
        "        for path_prefix in common_paths:\n",
        "            for csv_name in common_csv_names:\n",
        "                test_path = os.path.join(path_prefix, csv_name) if path_prefix else csv_name\n",
        "                if os.path.exists(test_path):\n",
        "                    csv_filepath = test_path\n",
        "                    print(f\"\\n‚úì Auto-detected CSV file: {csv_filepath}\")\n",
        "                    break\n",
        "            if csv_filepath:\n",
        "                break\n",
        "\n",
        "        # If still not found, prompt user\n",
        "        if not csv_filepath:\n",
        "            print(\"\\nüìÅ Please provide the path to your CSV file\")\n",
        "            print(\"   Example: US_Accidents_March23.csv\")\n",
        "            print(\"   Or: /content/US_Accidents_March23.csv\")\n",
        "            csv_filepath = input(\"Enter CSV file path: \").strip()\n",
        "\n",
        "            # Remove quotes if user added them\n",
        "            csv_filepath = csv_filepath.strip('\"').strip(\"'\")\n",
        "\n",
        "            if not csv_filepath:\n",
        "                print(\"‚ùå Error: No file path provided. Exiting.\")\n",
        "                sys.exit(1)\n",
        "\n",
        "    # Early validation - check if it looks like a wrong file\n",
        "    if 'kernel-' in csv_filepath and '.json' in csv_filepath:\n",
        "        print(f\"\\n‚ùå ERROR: The provided path looks like a Jupyter kernel file, not a CSV!\")\n",
        "        print(f\"   Path: {csv_filepath}\")\n",
        "        print(f\"\\n   This is likely a mistake. Please provide the path to your CSV file.\")\n",
        "        print(f\"   Example: US_Accidents_March23.csv\")\n",
        "        print(f\"   Or if the file is in your current directory, just use: US_Accidents_March23.csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Validate file exists\n",
        "    if not os.path.exists(csv_filepath):\n",
        "        print(f\"\\n‚ùå Error: File not found: {csv_filepath}\")\n",
        "        print(f\"\\n   Please check:\")\n",
        "        print(f\"   1. The file path is correct\")\n",
        "        print(f\"   2. The file exists in the specified location\")\n",
        "        print(f\"   3. You're in the correct directory\")\n",
        "        print(f\"\\n   If the CSV file is in your current directory, you can use just the filename.\")\n",
        "        print(f\"   Example: US_Accidents_March23.csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Validate file extension\n",
        "    if not csv_filepath.lower().endswith('.csv'):\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: File doesn't have .csv extension: {csv_filepath}\")\n",
        "        print(f\"   Continuing anyway, but make sure this is a CSV file...\")\n",
        "\n",
        "    # Validate sample fraction if provided\n",
        "    sample_fraction = args.sample\n",
        "    if sample_fraction is not None:\n",
        "        if sample_fraction <= 0 or sample_fraction > 1:\n",
        "            print(f\"‚ùå Error: Sample fraction must be between 0 and 1. Got: {sample_fraction}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    print(f\"\\n‚úì Using CSV file: {csv_filepath}\")\n",
        "    if sample_fraction:\n",
        "        print(f\"‚úì Sample fraction: {sample_fraction*100:.1f}%\")\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Step 1: Initialize Spark\n",
        "    print(\"\\nüöÄ Initializing Spark Session...\")\n",
        "    spark = None\n",
        "    try:\n",
        "        spark = create_spark_session()\n",
        "        print(\"‚úì Spark Session created successfully\")\n",
        "        print(f\"   Spark Version: {spark.version}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Failed to initialize Spark session: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        # Step 2: Load Data from CSV (with optional sampling)\n",
        "        df = load_data(spark, csv_filepath, sample_fraction=1)\n",
        "\n",
        "        # Step 3: Explore Data\n",
        "        df = explore_data(df)\n",
        "\n",
        "        # Step 4: Clean Data (ETL)\n",
        "        df_clean = clean_data(df)\n",
        "\n",
        "        # Step 5: Feature Engineering\n",
        "        df_features = engineer_features(df_clean)\n",
        "\n",
        "        # Step 6: Calculate Class Weights\n",
        "        df_weighted = calculate_class_weights(df_features)\n",
        "\n",
        "        # Cache the dataframe for faster processing\n",
        "        df_weighted.cache()\n",
        "\n",
        "        # Step 7: Build ML Pipeline\n",
        "        pipeline = build_ml_pipeline(df_weighted)\n",
        "\n",
        "        # Step 8: Train and Evaluate (with improved techniques)\n",
        "        # Set use_cross_validation=True for hyperparameter tuning (slower but better)\n",
        "        model, predictions, test_data = train_and_evaluate(df_weighted, pipeline, use_cross_validation=False)\n",
        "\n",
        "        # Step 9: Evaluate Model\n",
        "        metrics = evaluate_model(predictions)\n",
        "\n",
        "        # Step 10: Feature Importance\n",
        "        # Build feature names list dynamically\n",
        "        feature_names = [\n",
        "            \"Start_Lat\", \"Start_Lng\", \"Hour\", \"DayOfWeek\",\n",
        "            \"IsRushHour\", \"IsWeekend\", \"Temperature(F)\", \"Humidity(%)\",\n",
        "            \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\",\n",
        "            \"Crossing\", \"Junction\", \"Traffic_Signal\"\n",
        "        ]\n",
        "        if \"Weather_Condition\" in df_features.columns:\n",
        "            feature_names.append(\"Weather_Vec\")\n",
        "        if \"Sunrise_Sunset\" in df_features.columns:\n",
        "            feature_names.append(\"Sunrise_Vec\")\n",
        "\n",
        "        extract_feature_importance(model, feature_names)\n",
        "\n",
        "        # Step 11: Save Model\n",
        "        save_model(model)\n",
        "\n",
        "        # Summary\n",
        "        total_time = time.time() - total_start_time\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"   EXECUTION COMPLETE\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\n‚úì Total execution time: {total_time/60:.2f} minutes\")\n",
        "        print(f\"‚úì Final F1 Score: {metrics['f1_score']:.4f}\")\n",
        "        print(\"\\nüìå Key Takeaways:\")\n",
        "        print(\"   - Class weights were applied to handle imbalanced data\")\n",
        "        print(\"   - Random Forest with 50 trees was used for robustness\")\n",
        "        print(\"   - F1 Score is the primary metric for imbalanced classification\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark Session (only if it was created successfully)\n",
        "        if spark is not None:\n",
        "            try:\n",
        "                spark.stop()\n",
        "                print(\"\\n‚úì Spark Session stopped\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ö†Ô∏è  Warning: Error stopping Spark session: {e}\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  Spark Session was not created, skipping cleanup\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Traffic Accident Severity Prediction using Apache Spark MLlib\n",
        "=============================================================\n",
        "Predicts the Severity (1-4) of traffic accidents based on real-time factors.\n",
        "\n",
        "Big Data Challenge: Processing 7.7 million records using Apache Spark\n",
        "ML Challenge: Handling heavily imbalanced data using class weights\n",
        "\n",
        "IMPROVEMENTS FOR BETTER ACCURACY (Inspired by Kaggle best practices):\n",
        "- Using GBT (Gradient Boosting Trees) instead of Random Forest (XGBoost-style)\n",
        "- Enhanced feature engineering (TimeOfDay, Season, interactions)\n",
        "- Optimized hyperparameters (150 iterations, maxDepth=10, stepSize=0.05)\n",
        "- Improved class weight calculation for imbalanced data\n",
        "- Better feature selection and interaction terms\n",
        "\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, hour, dayofweek, when, lit, create_map, count\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
        "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from itertools import chain\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "def create_spark_session():\n",
        "    \"\"\"Initialize Spark Session with optimized configurations for large datasets.\"\"\"\n",
        "    import os\n",
        "\n",
        "    # Set Java options to avoid security manager issues\n",
        "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-java-options \"-Djava.security.manager=allow\" pyspark-shell'\n",
        "\n",
        "    try:\n",
        "        # Stop any existing Spark session (common issue in Colab/Jupyter)\n",
        "        try:\n",
        "            existing_spark = SparkSession.getActiveSession()\n",
        "            if existing_spark is not None:\n",
        "                print(\"WARNING: Stopping existing Spark session...\")\n",
        "                existing_spark.stop()\n",
        "                # Give it a moment to fully stop\n",
        "                import time\n",
        "                time.sleep(1)\n",
        "        except:\n",
        "            pass  # No existing session or error stopping it\n",
        "\n",
        "        # Try creating Spark session with full configuration\n",
        "        try:\n",
        "            spark = SparkSession.builder \\\n",
        "                .appName(\"AccidentSeverityPrediction\") \\\n",
        "                .config(\"spark.driver.memory\", \"20g\") \\\n",
        "                .config(\"spark.executor.memory\", \"20g\") \\\n",
        "                .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
        "                .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "                .config(\"spark.default.parallelism\", \"100\") \\\n",
        "                .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "                .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "                .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=200\") \\\n",
        "                .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=200\") \\\n",
        "                .master(\"local[*]\") \\\n",
        "                .getOrCreate()\n",
        "        except Exception as e1:\n",
        "            print(f\"WARNING: Failed with full config, trying simpler configuration...\")\n",
        "            # Fallback: Try with minimal configuration (common in Colab)\n",
        "            try:\n",
        "                spark = SparkSession.builder \\\n",
        "                    .appName(\"AccidentSeverityPrediction\") \\\n",
        "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .getOrCreate()\n",
        "                print(\"Created Spark session with minimal configuration\")\n",
        "            except Exception as e2:\n",
        "                # Last resort: Try with absolute minimum\n",
        "                print(f\"WARNING: Failed with minimal config, trying absolute minimum...\")\n",
        "                spark = SparkSession.builder \\\n",
        "                    .appName(\"AccidentSeverityPrediction\") \\\n",
        "                    .master(\"local[*]\") \\\n",
        "                    .getOrCreate()\n",
        "                print(\"Created Spark session with minimum configuration\")\n",
        "\n",
        "        # Validate that Spark session was created successfully\n",
        "        if spark is None:\n",
        "            raise RuntimeError(\"Failed to create Spark session - got None\")\n",
        "\n",
        "        # Validate sparkContext exists\n",
        "        if not hasattr(spark, 'sparkContext') or spark.sparkContext is None:\n",
        "            raise RuntimeError(\"Spark session created but sparkContext is None\")\n",
        "\n",
        "        # Set log level - use INFO to see stage progress, WARN to reduce verbosity\n",
        "        # INFO shows stage progress bars which are helpful for monitoring training\n",
        "        # In Colab, you'll see progress like: [Stage 95:==============================> (56 + 8) / 100]\n",
        "        spark.sparkContext.setLogLevel(\"INFO\")\n",
        "\n",
        "        return spark\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Error creating Spark session: {e}\")\n",
        "        raise\n",
        "\n",
        "def load_data(spark, filepath, sample_fraction=None):\n",
        "    \"\"\"Load CSV data into Spark DataFrame with schema inference.\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 1: DATA INGESTION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not filepath:\n",
        "        raise ValueError(\"CSV file path is required!\")\n",
        "\n",
        "    import os\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {filepath}\")\n",
        "\n",
        "    # Validate file extension\n",
        "    file_ext = os.path.splitext(filepath)[1].lower()\n",
        "    if file_ext not in ['.csv', '']:\n",
        "        print(f\"\\nWARNING: File extension is '{file_ext}', expected '.csv'\")\n",
        "        print(\"   The file might not be a CSV file. Continuing anyway...\")\n",
        "\n",
        "    # Check if file looks like a Jupyter kernel file (common mistake)\n",
        "    if 'kernel-' in filepath and filepath.endswith('.json'):\n",
        "        raise ValueError(\n",
        "            f\"\\nERROR: The file path appears to be a Jupyter kernel file, not a CSV file!\\n\"\n",
        "            f\"   File: {filepath}\\n\"\n",
        "            f\"   This looks like a Jupyter runtime file, not your data file.\\n\"\n",
        "            f\"   Please provide the path to your CSV file (e.g., 'US_Accidents_March23.csv')\\n\"\n",
        "            f\"   Example usage: python accident_severity_prediction.py US_Accidents_March23.csv\"\n",
        "        )\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Loading data from CSV file: {filepath}\")\n",
        "    df = spark.read.csv(filepath, header=True, inferSchema=True)\n",
        "\n",
        "    record_count = df.count()\n",
        "    column_count = len(df.columns)\n",
        "    column_names = df.columns\n",
        "\n",
        "    print(f\"Loaded {record_count:,} records with {column_count} columns\")\n",
        "\n",
        "    # Validate that this looks like a real CSV file\n",
        "    if column_count == 1 and column_names[0] in ['{', '_c0', 'value']:\n",
        "        raise ValueError(\n",
        "            f\"\\nERROR: The file doesn't appear to be a valid CSV file!\\n\"\n",
        "            f\"   File: {filepath}\\n\"\n",
        "            f\"   Found only 1 column: {column_names[0]}\\n\"\n",
        "            f\"   This might be a JSON file or incorrectly formatted CSV.\\n\"\n",
        "            f\"   Please check:\\n\"\n",
        "            f\"   1. The file is actually a CSV file\\n\"\n",
        "            f\"   2. The file has headers in the first row\\n\"\n",
        "            f\"   3. You're providing the correct file path\\n\"\n",
        "            f\"   Example: python accident_severity_prediction.py US_Accidents_March23.csv\"\n",
        "        )\n",
        "\n",
        "    # Show first few column names for verification\n",
        "    print(f\"\\nColumn names (first 10): {column_names[:10]}\")\n",
        "    if column_count > 10:\n",
        "        print(f\"   ... and {column_count - 10} more columns\")\n",
        "\n",
        "    # Check if Severity column exists (case-insensitive)\n",
        "    severity_found = any(col.lower() == \"severity\" for col in column_names)\n",
        "    if not severity_found:\n",
        "        print(\"\\nWARNING: No 'Severity' column found (case-insensitive check)\")\n",
        "        print(\"   The script will attempt to continue, but may fail if Severity column is missing.\")\n",
        "\n",
        "    # Apply sampling if requested\n",
        "    if sample_fraction is not None and 0 < sample_fraction < 1:\n",
        "        print(f\"\\nSampling {sample_fraction*100:.1f}% of data for faster training...\")\n",
        "        df = df.sample(fraction=sample_fraction, seed=42)\n",
        "        sampled_count = df.count()\n",
        "        print(f\"Sampled dataset: {sampled_count:,} records ({sample_fraction*100:.1f}% of original)\")\n",
        "\n",
        "    print(f\"Load time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def find_column_case_insensitive(df, column_name):\n",
        "    \"\"\"Find a column in DataFrame case-insensitively.\"\"\"\n",
        "    all_columns = [field.name for field in df.schema.fields]\n",
        "    for col_name in all_columns:\n",
        "        if col_name.lower() == column_name.lower():\n",
        "            return col_name\n",
        "    return None\n",
        "\n",
        "def explore_data(df):\n",
        "    \"\"\"Perform initial data exploration.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 2: DATA EXPLORATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Show schema\n",
        "    print(\"\\nDataset Schema (first 20 columns):\")\n",
        "    for field in df.schema.fields[:20]:\n",
        "        print(f\"   - {field.name}: {field.dataType}\")\n",
        "\n",
        "    # Show all columns if there are more than 20\n",
        "    if len(df.schema.fields) > 20:\n",
        "        print(f\"\\n   ... and {len(df.schema.fields) - 20} more columns\")\n",
        "\n",
        "    # Check if Severity column exists (case-insensitive)\n",
        "    all_columns = [field.name for field in df.schema.fields]\n",
        "    severity_col = find_column_case_insensitive(df, \"Severity\")\n",
        "\n",
        "    if severity_col is None:\n",
        "        print(\"\\nWARNING: 'Severity' column not found in the dataset!\")\n",
        "        print(\"\\nAvailable columns in the dataset:\")\n",
        "        for i, col_name in enumerate(all_columns, 1):\n",
        "            print(f\"   {i}. {col_name}\")\n",
        "        print(\"\\nERROR: The dataset must contain a 'Severity' column (case-insensitive).\")\n",
        "        print(\"   Please check your CSV file and ensure it has a 'Severity' column.\")\n",
        "        raise ValueError(f\"'Severity' column not found. Available columns: {all_columns[:10]}...\")\n",
        "\n",
        "    # Show severity distribution\n",
        "    print(f\"\\nSeverity Distribution (Target Variable) - using column '{severity_col}':\")\n",
        "    severity_dist = df.groupBy(severity_col).count().orderBy(severity_col)\n",
        "    severity_dist.show()\n",
        "\n",
        "    # Calculate class imbalance\n",
        "    total = df.count()\n",
        "    severity_counts = severity_dist.collect()\n",
        "    print(\"Class Imbalance Analysis:\")\n",
        "    for row in severity_counts:\n",
        "        pct = (row['count'] / total) * 100\n",
        "        print(f\"   Severity {row[severity_col]}: {row['count']:,} records ({pct:.2f}%)\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean and preprocess the data - ETL Pipeline.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 3: DATA CLEANING (ETL)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    initial_count = df.count()\n",
        "\n",
        "    # Find columns case-insensitively\n",
        "    column_mapping = {}\n",
        "    desired_columns = [\n",
        "        \"Severity\", \"Start_Lat\", \"Start_Lng\", \"Temperature(F)\",\n",
        "        \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\",\n",
        "        \"Weather_Condition\", \"Sunrise_Sunset\", \"Start_Time\",\n",
        "        \"Crossing\", \"Junction\", \"Traffic_Signal\"\n",
        "    ]\n",
        "\n",
        "    for desired_col in desired_columns:\n",
        "        found_col = find_column_case_insensitive(df, desired_col)\n",
        "        if found_col:\n",
        "            column_mapping[desired_col] = found_col\n",
        "\n",
        "    # Check for critical columns\n",
        "    if \"Severity\" not in column_mapping:\n",
        "        all_columns = [field.name for field in df.schema.fields]\n",
        "        print(f\"\\nERROR: 'Severity' column not found!\")\n",
        "        print(f\"Available columns: {all_columns}\")\n",
        "        raise ValueError(\"'Severity' column is required but not found in the dataset\")\n",
        "\n",
        "    # Select columns using their actual names (case-insensitive match)\n",
        "    existing_columns = list(column_mapping.values())\n",
        "    df_clean = df.select(existing_columns)\n",
        "\n",
        "    # Rename columns to standard names for easier processing\n",
        "    for desired_col, actual_col in column_mapping.items():\n",
        "        if actual_col != desired_col:\n",
        "            df_clean = df_clean.withColumnRenamed(actual_col, desired_col)\n",
        "\n",
        "    print(f\"Selected {len(existing_columns)} relevant columns\")\n",
        "\n",
        "    # Drop rows with null values in critical columns\n",
        "    critical_columns = [\"Severity\", \"Start_Lat\", \"Start_Lng\", \"Start_Time\"]\n",
        "    critical_columns = [c for c in critical_columns if c in df_clean.columns]\n",
        "    df_clean = df_clean.dropna(subset=critical_columns)\n",
        "\n",
        "    after_critical_drop = df_clean.count()\n",
        "    print(f\"Dropped {initial_count - after_critical_drop:,} rows with missing critical values\")\n",
        "\n",
        "    # Filter out invalid severity values (keep only 1-4)\n",
        "    df_clean = df_clean.filter((col(\"Severity\") >= 1) & (col(\"Severity\") <= 4))\n",
        "\n",
        "    # Convert severity to 0-based indexing (0, 1, 2, 3) for better ML compatibility\n",
        "    # Original: 1, 2, 3, 4 -> New: 0, 1, 2, 3\n",
        "    df_clean = df_clean.withColumn(\"Severity\", col(\"Severity\") - 1.0)\n",
        "\n",
        "    # Cast severity to double for ML\n",
        "    df_clean = df_clean.withColumn(\"Severity\", col(\"Severity\").cast(DoubleType()))\n",
        "\n",
        "    # Handle missing values in numerical columns with median imputation\n",
        "    numerical_cols = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\"]\n",
        "    numerical_cols = [c for c in numerical_cols if c in df_clean.columns]\n",
        "\n",
        "    if numerical_cols:\n",
        "        # Fill nulls with column means for simplicity (Imputer requires no nulls in output)\n",
        "        for col_name in numerical_cols:\n",
        "            mean_val = df_clean.select(col_name).agg({col_name: \"mean\"}).collect()[0][0]\n",
        "            if mean_val is not None:\n",
        "                df_clean = df_clean.fillna({col_name: mean_val})\n",
        "\n",
        "    # Handle missing categorical values\n",
        "    if \"Weather_Condition\" in df_clean.columns:\n",
        "        df_clean = df_clean.fillna({\"Weather_Condition\": \"Unknown\"})\n",
        "    if \"Sunrise_Sunset\" in df_clean.columns:\n",
        "        df_clean = df_clean.fillna({\"Sunrise_Sunset\": \"Day\"})\n",
        "\n",
        "    final_count = df_clean.count()\n",
        "    print(f\"Final clean dataset: {final_count:,} records\")\n",
        "    print(f\"Data retention rate: {(final_count/initial_count)*100:.2f}%\")\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def engineer_features(df):\n",
        "    \"\"\"Extract and engineer features from the cleaned data - Enhanced version.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 4: FEATURE ENGINEERING (ENHANCED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Extract temporal features from Start_Time\n",
        "    df = df.withColumn(\"Hour\", hour(col(\"Start_Time\")))\n",
        "    df = df.withColumn(\"DayOfWeek\", dayofweek(col(\"Start_Time\")))\n",
        "    df = df.withColumn(\"Month\", col(\"Start_Time\").substr(6, 2).cast(\"int\"))\n",
        "\n",
        "    print(\"Extracted Hour, DayOfWeek, and Month from timestamp\")\n",
        "\n",
        "    # Create rush hour indicator (7-9 AM and 4-7 PM)\n",
        "    df = df.withColumn(\"IsRushHour\",\n",
        "        when(((col(\"Hour\") >= 7) & (col(\"Hour\") <= 9)) |\n",
        "             ((col(\"Hour\") >= 16) & (col(\"Hour\") <= 19)), 1.0).otherwise(0.0))\n",
        "\n",
        "    print(\"Created IsRushHour feature\")\n",
        "\n",
        "    # Create weekend indicator\n",
        "    df = df.withColumn(\"IsWeekend\",\n",
        "        when((col(\"DayOfWeek\") == 1) | (col(\"DayOfWeek\") == 7), 1.0).otherwise(0.0))\n",
        "\n",
        "    print(\"Created IsWeekend feature\")\n",
        "\n",
        "    # Create time of day categories (Morning, Afternoon, Evening, Night)\n",
        "    df = df.withColumn(\"TimeOfDay\",\n",
        "        when((col(\"Hour\") >= 6) & (col(\"Hour\") < 12), 1.0)  # Morning\n",
        "        .when((col(\"Hour\") >= 12) & (col(\"Hour\") < 17), 2.0)  # Afternoon\n",
        "        .when((col(\"Hour\") >= 17) & (col(\"Hour\") < 22), 3.0)  # Evening\n",
        "        .otherwise(4.0))  # Night\n",
        "\n",
        "    print(\"Created TimeOfDay feature\")\n",
        "\n",
        "    # Create season feature (if Month available)\n",
        "    if \"Month\" in df.columns:\n",
        "        df = df.withColumn(\"Season\",\n",
        "            when((col(\"Month\") >= 3) & (col(\"Month\") <= 5), 1.0)  # Spring\n",
        "            .when((col(\"Month\") >= 6) & (col(\"Month\") <= 8), 2.0)  # Summer\n",
        "            .when((col(\"Month\") >= 9) & (col(\"Month\") <= 11), 3.0)  # Fall\n",
        "            .otherwise(4.0))  # Winter\n",
        "        print(\"Created Season feature\")\n",
        "\n",
        "    # Convert boolean columns to numeric\n",
        "    boolean_cols = [\"Crossing\", \"Junction\", \"Traffic_Signal\"]\n",
        "    for col_name in boolean_cols:\n",
        "        if col_name in df.columns:\n",
        "            df = df.withColumn(col_name,\n",
        "                when(col(col_name) == True, 1.0)\n",
        "                .when(col(col_name) == False, 0.0)\n",
        "                .otherwise(0.0))\n",
        "\n",
        "    print(\"Converted boolean features to numeric\")\n",
        "\n",
        "    # Create interaction features (important for XGBoost-style models)\n",
        "    if \"Temperature(F)\" in df.columns and \"Humidity(%)\" in df.columns:\n",
        "        df = df.withColumn(\"Temp_Humidity_Interaction\",\n",
        "            col(\"Temperature(F)\") * col(\"Humidity(%)\") / 100.0)\n",
        "        print(\"Created Temp_Humidity_Interaction feature\")\n",
        "\n",
        "    if \"Wind_Speed(mph)\" in df.columns and \"Visibility(mi)\" in df.columns:\n",
        "        df = df.withColumn(\"Wind_Visibility_Interaction\",\n",
        "            col(\"Wind_Speed(mph)\") / (col(\"Visibility(mi)\") + 0.1))  # Avoid division by zero\n",
        "        print(\"Created Wind_Visibility_Interaction feature\")\n",
        "\n",
        "    # Drop the original Start_Time column (no longer needed)\n",
        "    df = df.drop(\"Start_Time\")\n",
        "\n",
        "    # Show sample of engineered features\n",
        "    print(\"\\nSample of engineered features:\")\n",
        "    sample_cols = [\"Hour\", \"DayOfWeek\", \"IsRushHour\", \"IsWeekend\", \"TimeOfDay\"]\n",
        "    if \"Season\" in df.columns:\n",
        "        sample_cols.append(\"Season\")\n",
        "    df.select(sample_cols).show(5)\n",
        "\n",
        "    return df\n",
        "\n",
        "def calculate_class_weights(df):\n",
        "    \"\"\"Calculate class weights to handle imbalanced data - Improved method.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 5: HANDLING CLASS IMBALANCE (IMPROVED)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Calculate class distribution\n",
        "    class_counts = df.groupBy(\"Severity\").count().collect()\n",
        "\n",
        "    total_samples = sum([row[\"count\"] for row in class_counts])\n",
        "    num_classes = len(class_counts)\n",
        "\n",
        "    # Improved weight calculation: using sklearn's balanced method\n",
        "    # weight = n_samples / (n_classes * np.bincount(y))\n",
        "    # This gives more balanced weights\n",
        "    class_weights = {}\n",
        "    print(\"\\nClass Weights (Balanced - Improved):\")\n",
        "    print(\"Note: Severity is 0-based (0, 1, 2, 3) corresponding to original (1, 2, 3, 4)\")\n",
        "    for row in class_counts:\n",
        "        severity = row[\"Severity\"]\n",
        "        count = row[\"count\"]\n",
        "        # Balanced weight formula (similar to sklearn)\n",
        "        weight = total_samples / (num_classes * count)\n",
        "        class_weights[severity] = weight\n",
        "        pct = (count / total_samples) * 100\n",
        "        original_severity = int(severity) + 1  # Convert back to 1-4 for display\n",
        "        print(f\"   Severity {original_severity} (index {int(severity)}): {count:>12,} samples ({pct:>5.2f}%) -> weight = {weight:.4f}\")\n",
        "\n",
        "    # Create mapping expression for adding weights\n",
        "    mapping_expr = create_map([lit(x) for x in chain(*class_weights.items())])\n",
        "\n",
        "    # Add weight column to dataset\n",
        "    df_weighted = df.withColumn(\"classWeight\", mapping_expr[col(\"Severity\")])\n",
        "\n",
        "    print(\"\\nAdded classWeight column to handle imbalance\")\n",
        "    print(\"  Using balanced class weights for better minority class performance\")\n",
        "\n",
        "    return df_weighted\n",
        "\n",
        "def build_ml_pipeline(df):\n",
        "    \"\"\"Build the ML pipeline with feature transformers and classifier.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 6: BUILDING ML PIPELINE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    stages = []\n",
        "\n",
        "    # String Indexer for Weather_Condition\n",
        "    if \"Weather_Condition\" in df.columns:\n",
        "        weather_indexer = StringIndexer(\n",
        "            inputCol=\"Weather_Condition\",\n",
        "            outputCol=\"Weather_Index\",\n",
        "            handleInvalid=\"skip\"\n",
        "        )\n",
        "        weather_encoder = OneHotEncoder(\n",
        "            inputCols=[\"Weather_Index\"],\n",
        "            outputCols=[\"Weather_Vec\"]\n",
        "        )\n",
        "        stages.extend([weather_indexer, weather_encoder])\n",
        "        print(\"Added Weather_Condition encoder\")\n",
        "\n",
        "    # String Indexer for Sunrise_Sunset\n",
        "    if \"Sunrise_Sunset\" in df.columns:\n",
        "        sunrise_indexer = StringIndexer(\n",
        "            inputCol=\"Sunrise_Sunset\",\n",
        "            outputCol=\"Sunrise_Index\",\n",
        "            handleInvalid=\"skip\"\n",
        "        )\n",
        "        sunrise_encoder = OneHotEncoder(\n",
        "            inputCols=[\"Sunrise_Index\"],\n",
        "            outputCols=[\"Sunrise_Vec\"]\n",
        "        )\n",
        "        stages.extend([sunrise_indexer, sunrise_encoder])\n",
        "        print(\"Added Sunrise_Sunset encoder\")\n",
        "\n",
        "    # Define feature columns for the assembler\n",
        "    numerical_features = [\n",
        "        \"Start_Lat\", \"Start_Lng\", \"Hour\", \"DayOfWeek\",\n",
        "        \"IsRushHour\", \"IsWeekend\"\n",
        "    ]\n",
        "\n",
        "    # Add enhanced features if they exist\n",
        "    enhanced_features = [\"TimeOfDay\", \"Month\", \"Season\"]\n",
        "    for col_name in enhanced_features:\n",
        "        if col_name in df.columns:\n",
        "            numerical_features.append(col_name)\n",
        "\n",
        "    # Add optional numerical features if they exist\n",
        "    optional_numerical = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\",\n",
        "                          \"Visibility(mi)\", \"Wind_Speed(mph)\",\n",
        "                          \"Crossing\", \"Junction\", \"Traffic_Signal\",\n",
        "                          \"Temp_Humidity_Interaction\", \"Wind_Visibility_Interaction\"]\n",
        "\n",
        "    for col_name in optional_numerical:\n",
        "        if col_name in df.columns:\n",
        "            numerical_features.append(col_name)\n",
        "\n",
        "    # Build final feature list\n",
        "    feature_cols = numerical_features.copy()\n",
        "    if \"Weather_Condition\" in df.columns:\n",
        "        feature_cols.append(\"Weather_Vec\")\n",
        "    if \"Sunrise_Sunset\" in df.columns:\n",
        "        feature_cols.append(\"Sunrise_Vec\")\n",
        "\n",
        "    print(f\"Feature columns: {len(feature_cols)} features\")\n",
        "\n",
        "    # Vector Assembler\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=feature_cols,\n",
        "        outputCol=\"features\",\n",
        "        handleInvalid=\"skip\"\n",
        "    )\n",
        "    stages.append(assembler)\n",
        "    print(\"Added VectorAssembler\")\n",
        "\n",
        "    # Build feature preparation pipeline (without classifier)\n",
        "    # We'll use One-vs-Rest with GBTClassifier, so we don't add the classifier here\n",
        "    feature_pipeline = Pipeline(stages=stages)\n",
        "\n",
        "    print(\"Built feature preparation pipeline\")\n",
        "    print(\"  Will use One-vs-Rest with GBTClassifier for multiclass classification\")\n",
        "\n",
        "    return feature_pipeline\n",
        "\n",
        "def train_and_evaluate_ovr(df, feature_pipeline):\n",
        "    \"\"\"Train One-vs-Rest GBTClassifier models and evaluate performance.\"\"\"\n",
        "    from pyspark.sql.functions import when, col\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 7: MODEL TRAINING (One-vs-Rest with GBTClassifier)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Split data: 80% train, 20% test\n",
        "    train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    train_count = train_data.count()\n",
        "    test_count = test_data.count()\n",
        "\n",
        "    print(f\"Training set: {train_count:,} records\")\n",
        "    print(f\"Test set: {test_count:,} records\")\n",
        "\n",
        "    # Apply feature pipeline to prepare features\n",
        "    print(\"\\nPreparing features...\")\n",
        "    feature_model = feature_pipeline.fit(train_data)\n",
        "    train_features = feature_model.transform(train_data)\n",
        "    test_features = feature_model.transform(test_data)\n",
        "\n",
        "    # Cache feature dataframes\n",
        "    train_features.cache()\n",
        "    test_features.cache()\n",
        "\n",
        "    # Train 4 GBTClassifier models using One-vs-Rest\n",
        "    models = {}\n",
        "    severity_levels = [0, 1, 2, 3]  # 0-based severity levels\n",
        "\n",
        "    print(\"\\nTraining 4 GBTClassifier models (One-vs-Rest)...\")\n",
        "    print(\"   This will take longer as we're training 4 separate models...\")\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for severity in severity_levels:\n",
        "        print(f\"\\n   Training model for Severity {severity} (vs. All Others)...\")\n",
        "\n",
        "        # Create binary label: 1 if this severity, 0 otherwise\n",
        "        binary_train = train_features.withColumn(\n",
        "            \"binary_label\",\n",
        "            when(col(\"Severity\") == severity, 1.0).otherwise(0.0)\n",
        "        )\n",
        "\n",
        "        # Calculate class weights for this binary problem\n",
        "        pos_count = binary_train.filter(col(\"binary_label\") == 1.0).count()\n",
        "        neg_count = binary_train.filter(col(\"binary_label\") == 0.0).count()\n",
        "        total_count = pos_count + neg_count\n",
        "\n",
        "        if pos_count > 0 and neg_count > 0:\n",
        "            # Calculate balanced weights\n",
        "            pos_weight = total_count / (2.0 * pos_count)\n",
        "            neg_weight = total_count / (2.0 * neg_count)\n",
        "\n",
        "            binary_train = binary_train.withColumn(\n",
        "                \"binary_classWeight\",\n",
        "                when(col(\"binary_label\") == 1.0, pos_weight).otherwise(neg_weight)\n",
        "            )\n",
        "\n",
        "            print(f\"      Positive samples: {pos_count:,} (weight: {pos_weight:.4f})\")\n",
        "            print(f\"      Negative samples: {neg_count:,} (weight: {neg_weight:.4f})\")\n",
        "        else:\n",
        "            binary_train = binary_train.withColumn(\"binary_classWeight\", col(\"classWeight\"))\n",
        "            print(f\"      Using original class weights\")\n",
        "\n",
        "        # Train GBTClassifier for this binary problem\n",
        "        gbt = GBTClassifier(\n",
        "            labelCol=\"binary_label\",\n",
        "            featuresCol=\"features\",\n",
        "            weightCol=\"binary_classWeight\",\n",
        "            maxIter=50,  # Number of boosting iterations\n",
        "            maxDepth=12,  # Maximum depth of trees\n",
        "            stepSize=0.1,  # Learning rate\n",
        "            minInstancesPerNode=10,\n",
        "            seed=42,\n",
        "            maxBins=32\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "        model = gbt.fit(binary_train)\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        models[severity] = model\n",
        "        print(f\"      Model {severity} trained in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "\n",
        "    total_training_time = time.time() - total_start_time\n",
        "    print(f\"\\nAll 4 models trained in {total_training_time:.2f} seconds ({total_training_time/60:.2f} minutes)\")\n",
        "\n",
        "    # Make predictions using all 4 models\n",
        "    print(\"\\nMaking predictions using One-vs-Rest ensemble...\")\n",
        "\n",
        "    # Add a unique row identifier to test_features for joining\n",
        "    from pyspark.sql.functions import monotonically_increasing_id\n",
        "    test_features_with_id = test_features.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "\n",
        "    # Get probability predictions from each model\n",
        "    prediction_dfs = []\n",
        "    for severity in severity_levels:\n",
        "        binary_test = test_features_with_id.withColumn(\n",
        "            \"binary_label\",\n",
        "            when(col(\"Severity\") == severity, 1.0).otherwise(0.0)\n",
        "        )\n",
        "\n",
        "        # Get probability of positive class (this severity)\n",
        "        pred = models[severity].transform(binary_test)\n",
        "        # Extract probability of class 1 (positive)\n",
        "        from pyspark.sql.functions import udf\n",
        "        from pyspark.sql.types import DoubleType\n",
        "\n",
        "        get_prob = udf(lambda v: float(v[1]), DoubleType())\n",
        "        pred = pred.withColumn(f\"prob_severity_{severity}\", get_prob(\"probability\"))\n",
        "        prediction_dfs.append(pred.select(\"row_id\", \"Severity\", f\"prob_severity_{severity}\"))\n",
        "\n",
        "    # Combine all probability predictions using row_id\n",
        "    combined = prediction_dfs[0]\n",
        "    for i in range(1, len(prediction_dfs)):\n",
        "        combined = combined.join(prediction_dfs[i], on=\"row_id\", how=\"inner\")\n",
        "\n",
        "    # Find the severity with highest probability\n",
        "    from pyspark.sql.functions import greatest, when as spark_when\n",
        "\n",
        "    prob_cols = [col(f\"prob_severity_{s}\") for s in severity_levels]\n",
        "    max_prob = greatest(*prob_cols)\n",
        "\n",
        "    # Determine prediction based on highest probability\n",
        "    prediction_expr = spark_when(\n",
        "        col(\"prob_severity_0\") == max_prob, 0.0\n",
        "    ).when(\n",
        "        col(\"prob_severity_1\") == max_prob, 1.0\n",
        "    ).when(\n",
        "        col(\"prob_severity_2\") == max_prob, 2.0\n",
        "    ).otherwise(3.0)\n",
        "\n",
        "    combined = combined.withColumn(\"prediction\", prediction_expr)\n",
        "\n",
        "    # Join back with test_features to get all columns\n",
        "    predictions = test_features_with_id.join(\n",
        "        combined.select(\"row_id\", \"prediction\"),\n",
        "        on=\"row_id\",\n",
        "        how=\"inner\"\n",
        "    ).drop(\"row_id\")\n",
        "\n",
        "    # Unpersist cached data\n",
        "    train_features.unpersist()\n",
        "    test_features.unpersist()\n",
        "\n",
        "    # Return models dict, feature_model, predictions, and test_data\n",
        "    return models, feature_model, predictions, test_data\n",
        "\n",
        "def evaluate_model(predictions):\n",
        "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 8: MODEL EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"accuracy\"\n",
        "    )\n",
        "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Weighted Precision\n",
        "    precision_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"weightedPrecision\"\n",
        "    )\n",
        "    precision = precision_evaluator.evaluate(predictions)\n",
        "\n",
        "    # Weighted Recall\n",
        "    recall_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"weightedRecall\"\n",
        "    )\n",
        "    recall = recall_evaluator.evaluate(predictions)\n",
        "\n",
        "    # F1 Score (most important for imbalanced data)\n",
        "    f1_evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"Severity\",\n",
        "        predictionCol=\"prediction\",\n",
        "        metricName=\"f1\"\n",
        "    )\n",
        "    f1_score = f1_evaluator.evaluate(predictions)\n",
        "\n",
        "    print(\"\\nMODEL PERFORMANCE METRICS:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"   Accuracy:           {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"   Weighted Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"   Weighted Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "    print(f\"   F1 Score:           {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Per-class metrics\n",
        "    print(\"\\nConfusion Matrix Analysis:\")\n",
        "    predictions.groupBy(\"Severity\", \"prediction\").count().orderBy(\"Severity\", \"prediction\").show(20)\n",
        "\n",
        "    # Prediction distribution\n",
        "    print(\"\\nPrediction Distribution:\")\n",
        "    predictions.groupBy(\"prediction\").count().orderBy(\"prediction\").show()\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "\n",
        "def extract_feature_importance(models_dict, feature_names):\n",
        "    \"\"\"Extract and display feature importance from all trained models (averaged).\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 9: FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Collect importances from all models\n",
        "        all_importances = []\n",
        "        for severity, model in models_dict.items():\n",
        "            if hasattr(model, 'featureImportances'):\n",
        "                importances = model.featureImportances.toArray()\n",
        "                all_importances.append(importances)\n",
        "            else:\n",
        "                print(f\"WARNING: Model for Severity {severity} doesn't support feature importance extraction\")\n",
        "\n",
        "        if not all_importances:\n",
        "            print(\"WARNING: No models support feature importance extraction\")\n",
        "            return None\n",
        "\n",
        "        # Average importances across all models\n",
        "        avg_importances = np.mean(all_importances, axis=0)\n",
        "\n",
        "        # Create feature importance pairs\n",
        "        feature_importance = list(zip(feature_names, avg_importances))\n",
        "\n",
        "        # Sort by importance (descending)\n",
        "        feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(\"\\nTop 20 Most Important Features (Averaged across all 4 models):\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, (feature, importance) in enumerate(feature_importance[:20], 1):\n",
        "            bar = \"‚ñà\" * int(importance * 100)\n",
        "            print(f\"{i:2}. {feature:30} {importance:.4f} {bar}\")\n",
        "\n",
        "        return feature_importance\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not extract feature importance: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def save_model(models_dict, feature_model, base_path=\"accident_severity_model\"):\n",
        "    \"\"\"Save all trained models (feature pipeline + 4 GBTClassifier models) to disk.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 10: SAVING MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Save feature preparation pipeline\n",
        "        feature_path = f\"{base_path}_features\"\n",
        "        feature_model.write().overwrite().save(feature_path)\n",
        "        print(f\"Feature pipeline saved to: {feature_path}\")\n",
        "\n",
        "        # Save each GBTClassifier model\n",
        "        for severity, model in models_dict.items():\n",
        "            model_path = f\"{base_path}_severity_{severity}\"\n",
        "            model.write().overwrite().save(model_path)\n",
        "            print(f\"Model for Severity {severity} saved to: {model_path}\")\n",
        "\n",
        "        print(f\"\\nAll models saved successfully!\")\n",
        "        print(f\"   Base path: {base_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not save models: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    import sys\n",
        "    import os\n",
        "\n",
        "    # Filter out IPython/Colab magic command flags (like -f) and kernel files before parsing\n",
        "    # These flags are used by %run magic command and shouldn't be passed to the script\n",
        "    filtered_argv = [sys.argv[0]]  # Keep script name\n",
        "    for arg in sys.argv[1:]:\n",
        "        # Skip magic command flags\n",
        "        if arg in ['-f', '-i', '-e', '-t', '-N', '-n', '-p']:\n",
        "            continue\n",
        "        # Skip kernel JSON files (common mistake in Colab)\n",
        "        if 'kernel-' in arg and arg.endswith('.json'):\n",
        "            print(f\"WARNING: Ignoring kernel file argument: {arg}\")\n",
        "            continue\n",
        "        # Skip if it looks like a kernel file path\n",
        "        if '/jupyter/runtime/kernel-' in arg:\n",
        "            print(f\"WARNING: Ignoring kernel file path: {arg}\")\n",
        "            continue\n",
        "        filtered_argv.append(arg)\n",
        "\n",
        "    original_argv = sys.argv\n",
        "    sys.argv = filtered_argv\n",
        "\n",
        "    # Parse command line arguments\n",
        "    parser = argparse.ArgumentParser(description='Traffic Accident Severity Prediction using Apache Spark MLlib')\n",
        "    parser.add_argument('csv_file', nargs='?', help='Path to the CSV file containing accident data')\n",
        "    parser.add_argument('--sample', type=float, default=None,\n",
        "                       help='Sample fraction (0.0 to 1.0) to use for training. Example: 0.1 for 10%%')\n",
        "\n",
        "    # Use parse_known_args to ignore any remaining unknown arguments (Colab compatibility)\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    # Restore original argv\n",
        "    sys.argv = original_argv\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"   TRAFFIC ACCIDENT SEVERITY PREDICTION SYSTEM\")\n",
        "    print(\"   Using Apache Spark MLlib for Big Data Processing\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Get CSV file path from command line argument or prompt\n",
        "    if args.csv_file:\n",
        "        csv_filepath = args.csv_file\n",
        "    else:\n",
        "        # Try to auto-detect CSV file in common Colab locations\n",
        "        common_csv_names = [\n",
        "            'US_Accidents_March23.csv',\n",
        "            'US_Accidents.csv',\n",
        "            'accidents.csv',\n",
        "            'data.csv'\n",
        "        ]\n",
        "        common_paths = [\n",
        "            '/content/',\n",
        "            './',\n",
        "            ''\n",
        "        ]\n",
        "\n",
        "        csv_filepath = None\n",
        "        for path_prefix in common_paths:\n",
        "            for csv_name in common_csv_names:\n",
        "                test_path = os.path.join(path_prefix, csv_name) if path_prefix else csv_name\n",
        "                if os.path.exists(test_path):\n",
        "                    csv_filepath = test_path\n",
        "                    print(f\"\\nAuto-detected CSV file: {csv_filepath}\")\n",
        "                    break\n",
        "            if csv_filepath:\n",
        "                break\n",
        "\n",
        "        # If still not found, prompt user\n",
        "        if not csv_filepath:\n",
        "            print(\"\\nüìÅ Please provide the path to your CSV file\")\n",
        "            print(\"   Example: US_Accidents_March23.csv\")\n",
        "            print(\"   Or: /content/US_Accidents_March23.csv\")\n",
        "            csv_filepath = input(\"Enter CSV file path: \").strip()\n",
        "\n",
        "            # Remove quotes if user added them\n",
        "            csv_filepath = csv_filepath.strip('\"').strip(\"'\")\n",
        "\n",
        "            if not csv_filepath:\n",
        "                print(\"ERROR: No file path provided. Exiting.\")\n",
        "                sys.exit(1)\n",
        "\n",
        "    # Early validation - check if it looks like a wrong file\n",
        "    if 'kernel-' in csv_filepath and '.json' in csv_filepath:\n",
        "        print(f\"\\nERROR: The provided path looks like a Jupyter kernel file, not a CSV!\")\n",
        "        print(f\"   Path: {csv_filepath}\")\n",
        "        print(f\"\\n   This is likely a mistake. Please provide the path to your CSV file.\")\n",
        "        print(f\"   Example: US_Accidents_March23.csv\")\n",
        "        print(f\"   Or if the file is in your current directory, just use: US_Accidents_March23.csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Validate file exists\n",
        "    if not os.path.exists(csv_filepath):\n",
        "        print(f\"\\nERROR: File not found: {csv_filepath}\")\n",
        "        print(f\"\\n   Please check:\")\n",
        "        print(f\"   1. The file path is correct\")\n",
        "        print(f\"   2. The file exists in the specified location\")\n",
        "        print(f\"   3. You're in the correct directory\")\n",
        "        print(f\"\\n   If the CSV file is in your current directory, you can use just the filename.\")\n",
        "        print(f\"   Example: US_Accidents_March23.csv\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Validate file extension\n",
        "    if not csv_filepath.lower().endswith('.csv'):\n",
        "        print(f\"\\nWARNING: File doesn't have .csv extension: {csv_filepath}\")\n",
        "        print(f\"   Continuing anyway, but make sure this is a CSV file...\")\n",
        "\n",
        "    # Validate sample fraction if provided\n",
        "    sample_fraction = args.sample\n",
        "    if sample_fraction is not None:\n",
        "        if sample_fraction <= 0 or sample_fraction > 1:\n",
        "            print(f\"ERROR: Sample fraction must be between 0 and 1. Got: {sample_fraction}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    print(f\"\\nUsing CSV file: {csv_filepath}\")\n",
        "    if sample_fraction:\n",
        "        print(f\"Sample fraction: {sample_fraction*100:.1f}%\")\n",
        "\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # Step 1: Initialize Spark\n",
        "    print(\"\\nInitializing Spark Session...\")\n",
        "    spark = None\n",
        "    try:\n",
        "        spark = create_spark_session()\n",
        "        print(\"Spark Session created successfully\")\n",
        "        print(f\"   Spark Version: {spark.version}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Failed to initialize Spark session: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    try:\n",
        "        # Step 2: Load Data from CSV (with optional sampling)\n",
        "        df = load_data(spark, csv_filepath, sample_fraction=0.6)\n",
        "\n",
        "        # Step 3: Explore Data\n",
        "        df = explore_data(df)\n",
        "\n",
        "        # Step 4: Clean Data (ETL)\n",
        "        df_clean = clean_data(df)\n",
        "\n",
        "        # Step 5: Feature Engineering\n",
        "        df_features = engineer_features(df_clean)\n",
        "\n",
        "        # Step 6: Calculate Class Weights\n",
        "        df_weighted = calculate_class_weights(df_features)\n",
        "\n",
        "        # Cache the dataframe for faster processing\n",
        "        df_weighted.cache()\n",
        "\n",
        "        # Step 7: Build Feature Preparation Pipeline\n",
        "        feature_pipeline = build_ml_pipeline(df_weighted)\n",
        "\n",
        "        # Step 8: Train and Evaluate using One-vs-Rest with GBTClassifier\n",
        "        models_dict, feature_model, predictions, test_data = train_and_evaluate_ovr(df_weighted, feature_pipeline)\n",
        "\n",
        "        # Step 9: Evaluate Model\n",
        "        metrics = evaluate_model(predictions)\n",
        "\n",
        "        # Step 10: Feature Importance\n",
        "        # Build feature names list dynamically\n",
        "        feature_names = [\n",
        "            \"Start_Lat\", \"Start_Lng\", \"Hour\", \"DayOfWeek\",\n",
        "            \"IsRushHour\", \"IsWeekend\", \"Temperature(F)\", \"Humidity(%)\",\n",
        "            \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\",\n",
        "            \"Crossing\", \"Junction\", \"Traffic_Signal\"\n",
        "        ]\n",
        "        if \"Weather_Condition\" in df_features.columns:\n",
        "            feature_names.append(\"Weather_Vec\")\n",
        "        if \"Sunrise_Sunset\" in df_features.columns:\n",
        "            feature_names.append(\"Sunrise_Vec\")\n",
        "\n",
        "        extract_feature_importance(models_dict, feature_names)\n",
        "\n",
        "        # Step 11: Save Models\n",
        "        save_model(models_dict, feature_model)\n",
        "\n",
        "        # Summary\n",
        "        total_time = time.time() - total_start_time\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"   EXECUTION COMPLETE\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\nTotal execution time: {total_time/60:.2f} minutes\")\n",
        "        print(f\"Final F1 Score: {metrics['f1_score']:.4f}\")\n",
        "        print(\"\\nKey Takeaways:\")\n",
        "        print(\"   - One-vs-Rest approach with GBTClassifier (XGBoost-like)\")\n",
        "        print(\"   - 4 binary GBTClassifier models trained (one per severity level)\")\n",
        "        print(\"   - Class weights applied to each binary model\")\n",
        "        print(\"   - Predictions combined using highest probability\")\n",
        "        print(\"   - F1 Score is the primary metric for imbalanced classification\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: Error during execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        # Stop Spark Session (only if it was created successfully)\n",
        "        if spark is not None:\n",
        "            try:\n",
        "                spark.stop()\n",
        "                print(\"\\nSpark Session stopped\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\nWARNING: Error stopping Spark session: {e}\")\n",
        "        else:\n",
        "            print(\"\\nWARNING: Spark Session was not created, skipping cleanup\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNLyGMdB5Cw0",
        "outputId": "55e145da-08f5-4114-dd03-f3f71c11d45e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Ignoring kernel file argument: /root/.local/share/jupyter/runtime/kernel-e721a625-bd3b-4276-9188-fbd49c1c5479.json\n",
            "\n",
            "======================================================================\n",
            "   TRAFFIC ACCIDENT SEVERITY PREDICTION SYSTEM\n",
            "   Using Apache Spark MLlib for Big Data Processing\n",
            "======================================================================\n",
            "\n",
            "Auto-detected CSV file: /content/US_Accidents_March23.csv\n",
            "\n",
            "Using CSV file: /content/US_Accidents_March23.csv\n",
            "\n",
            "Initializing Spark Session...\n",
            "Spark Session created successfully\n",
            "   Spark Version: 3.5.1\n",
            "============================================================\n",
            "STEP 1: DATA INGESTION\n",
            "============================================================\n",
            "Loading data from CSV file: /content/US_Accidents_March23.csv\n",
            "Loaded 7,728,394 records with 46 columns\n",
            "\n",
            "Column names (first 10): ['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)']\n",
            "   ... and 36 more columns\n",
            "\n",
            "Sampling 60.0% of data for faster training...\n",
            "Sampled dataset: 4,636,775 records (60.0% of original)\n",
            "Load time: 34.52 seconds\n",
            "\n",
            "============================================================\n",
            "STEP 2: DATA EXPLORATION\n",
            "============================================================\n",
            "\n",
            "Dataset Schema (first 20 columns):\n",
            "   - ID: StringType()\n",
            "   - Source: StringType()\n",
            "   - Severity: IntegerType()\n",
            "   - Start_Time: TimestampType()\n",
            "   - End_Time: TimestampType()\n",
            "   - Start_Lat: DoubleType()\n",
            "   - Start_Lng: DoubleType()\n",
            "   - End_Lat: DoubleType()\n",
            "   - End_Lng: DoubleType()\n",
            "   - Distance(mi): DoubleType()\n",
            "   - Description: StringType()\n",
            "   - Street: StringType()\n",
            "   - City: StringType()\n",
            "   - County: StringType()\n",
            "   - State: StringType()\n",
            "   - Zipcode: StringType()\n",
            "   - Country: StringType()\n",
            "   - Timezone: StringType()\n",
            "   - Airport_Code: StringType()\n",
            "   - Weather_Timestamp: TimestampType()\n",
            "\n",
            "   ... and 26 more columns\n",
            "\n",
            "Severity Distribution (Target Variable) - using column 'Severity':\n",
            "+--------+-------+\n",
            "|Severity|  count|\n",
            "+--------+-------+\n",
            "|       1|  40440|\n",
            "|       2|3693751|\n",
            "|       3| 779676|\n",
            "|       4| 122908|\n",
            "+--------+-------+\n",
            "\n",
            "Class Imbalance Analysis:\n",
            "   Severity 1: 40,440 records (0.87%)\n",
            "   Severity 2: 3,693,751 records (79.66%)\n",
            "   Severity 3: 779,676 records (16.82%)\n",
            "   Severity 4: 122,908 records (2.65%)\n",
            "\n",
            "============================================================\n",
            "STEP 3: DATA CLEANING (ETL)\n",
            "============================================================\n",
            "Selected 14 relevant columns\n",
            "Dropped 0 rows with missing critical values\n",
            "Final clean dataset: 4,636,775 records\n",
            "Data retention rate: 100.00%\n",
            "\n",
            "============================================================\n",
            "STEP 4: FEATURE ENGINEERING (ENHANCED)\n",
            "============================================================\n",
            "Extracted Hour, DayOfWeek, and Month from timestamp\n",
            "Created IsRushHour feature\n",
            "Created IsWeekend feature\n",
            "Created TimeOfDay feature\n",
            "Created Season feature\n",
            "Converted boolean features to numeric\n",
            "Created Temp_Humidity_Interaction feature\n",
            "Created Wind_Visibility_Interaction feature\n",
            "\n",
            "Sample of engineered features:\n",
            "+----+---------+----------+---------+---------+------+\n",
            "|Hour|DayOfWeek|IsRushHour|IsWeekend|TimeOfDay|Season|\n",
            "+----+---------+----------+---------+---------+------+\n",
            "|   6|        2|       0.0|      0.0|      1.0|   4.0|\n",
            "|   7|        2|       1.0|      0.0|      1.0|   4.0|\n",
            "|   7|        2|       1.0|      0.0|      1.0|   4.0|\n",
            "|   7|        2|       1.0|      0.0|      1.0|   4.0|\n",
            "|   8|        2|       1.0|      0.0|      1.0|   4.0|\n",
            "+----+---------+----------+---------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "============================================================\n",
            "STEP 5: HANDLING CLASS IMBALANCE (IMPROVED)\n",
            "============================================================\n",
            "\n",
            "Class Weights (Balanced - Improved):\n",
            "Note: Severity is 0-based (0, 1, 2, 3) corresponding to original (1, 2, 3, 4)\n",
            "   Severity 1 (index 0):       40,440 samples ( 0.87%) -> weight = 28.6645\n",
            "   Severity 2 (index 1):    3,693,751 samples (79.66%) -> weight = 0.3138\n",
            "   Severity 4 (index 3):      122,908 samples ( 2.65%) -> weight = 9.4314\n",
            "   Severity 3 (index 2):      779,676 samples (16.82%) -> weight = 1.4868\n",
            "\n",
            "Added classWeight column to handle imbalance\n",
            "  Using balanced class weights for better minority class performance\n",
            "\n",
            "============================================================\n",
            "STEP 6: BUILDING ML PIPELINE\n",
            "============================================================\n",
            "Added Weather_Condition encoder\n",
            "Added Sunrise_Sunset encoder\n",
            "Feature columns: 21 features\n",
            "Added VectorAssembler\n",
            "Built feature preparation pipeline\n",
            "  Will use One-vs-Rest with GBTClassifier for multiclass classification\n",
            "\n",
            "============================================================\n",
            "STEP 7: MODEL TRAINING (One-vs-Rest with GBTClassifier)\n",
            "============================================================\n",
            "Training set: 3,708,212 records\n",
            "Test set: 928,563 records\n",
            "\n",
            "Preparing features...\n",
            "\n",
            "Training 4 GBTClassifier models (One-vs-Rest)...\n",
            "   This will take longer as we're training 4 separate models...\n",
            "\n",
            "   Training model for Severity 0 (vs. All Others)...\n",
            "      Positive samples: 32,449 (weight: 57.1391)\n",
            "      Negative samples: 3,675,763 (weight: 0.5044)\n",
            "      Model 0 trained in 4069.06 seconds (67.82 minutes)\n",
            "\n",
            "   Training model for Severity 1 (vs. All Others)...\n",
            "      Positive samples: 2,954,036 (weight: 0.6277)\n",
            "      Negative samples: 754,176 (weight: 2.4585)\n",
            "      Model 1 trained in 3647.67 seconds (60.79 minutes)\n",
            "\n",
            "   Training model for Severity 2 (vs. All Others)...\n",
            "      Positive samples: 623,188 (weight: 2.9752)\n",
            "      Negative samples: 3,085,024 (weight: 0.6010)\n",
            "      Model 2 trained in 3814.06 seconds (63.57 minutes)\n",
            "\n",
            "   Training model for Severity 3 (vs. All Others)...\n",
            "      Positive samples: 98,539 (weight: 18.8160)\n",
            "      Negative samples: 3,609,673 (weight: 0.5136)\n",
            "      Model 3 trained in 3562.11 seconds (59.37 minutes)\n",
            "\n",
            "All 4 models trained in 15111.65 seconds (251.86 minutes)\n",
            "\n",
            "Making predictions using One-vs-Rest ensemble...\n",
            "\n",
            "============================================================\n",
            "STEP 8: MODEL EVALUATION\n",
            "============================================================\n",
            "\n",
            "MODEL PERFORMANCE METRICS:\n",
            "----------------------------------------\n",
            "   Accuracy:           0.6137 (61.37%)\n",
            "   Weighted Precision: 0.8051 (80.51%)\n",
            "   Weighted Recall:    0.6137 (61.37%)\n",
            "   F1 Score:           0.6733 (67.33%)\n",
            "----------------------------------------\n",
            "\n",
            "Confusion Matrix Analysis:\n",
            "+--------+----------+------+\n",
            "|Severity|prediction| count|\n",
            "+--------+----------+------+\n",
            "|     0.0|       0.0|  6351|\n",
            "|     0.0|       1.0|   993|\n",
            "|     0.0|       2.0|   390|\n",
            "|     0.0|       3.0|   257|\n",
            "|     1.0|       0.0| 64661|\n",
            "|     1.0|       1.0|451925|\n",
            "|     1.0|       2.0|153459|\n",
            "|     1.0|       3.0| 69666|\n",
            "|     2.0|       0.0| 10808|\n",
            "|     2.0|       1.0| 30265|\n",
            "|     2.0|       2.0|100102|\n",
            "|     2.0|       3.0| 15312|\n",
            "|     3.0|       0.0|  1414|\n",
            "|     3.0|       1.0|  5697|\n",
            "|     3.0|       2.0|  5795|\n",
            "|     3.0|       3.0| 11463|\n",
            "+--------+----------+------+\n",
            "\n",
            "\n",
            "Prediction Distribution:\n",
            "+----------+------+\n",
            "|prediction| count|\n",
            "+----------+------+\n",
            "|       0.0| 83234|\n",
            "|       1.0|488880|\n",
            "|       2.0|259746|\n",
            "|       3.0| 96698|\n",
            "+----------+------+\n",
            "\n",
            "\n",
            "============================================================\n",
            "STEP 9: FEATURE IMPORTANCE ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Top 20 Most Important Features (Averaged across all 4 models):\n",
            "------------------------------------------------------------\n",
            " 1. Start_Lat                      0.1556 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " 2. Start_Lng                      0.1549 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " 3. Crossing                       0.0864 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " 4. Pressure(in)                   0.0709 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " 5. Hour                           0.0621 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " 6. Traffic_Signal                 0.0505 ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
            " 7. Visibility(mi)                 0.0483 ‚ñà‚ñà‚ñà‚ñà\n",
            " 8. Humidity(%)                    0.0419 ‚ñà‚ñà‚ñà‚ñà\n",
            " 9. Wind_Speed(mph)                0.0391 ‚ñà‚ñà‚ñà\n",
            "10. DayOfWeek                      0.0253 ‚ñà‚ñà\n",
            "11. Weather_Vec                    0.0167 ‚ñà\n",
            "12. IsWeekend                      0.0146 ‚ñà\n",
            "13. Junction                       0.0140 ‚ñà\n",
            "14. Temperature(F)                 0.0126 ‚ñà\n",
            "15. Sunrise_Vec                    0.0100 \n",
            "16. IsRushHour                     0.0061 \n",
            "\n",
            "============================================================\n",
            "STEP 10: SAVING MODELS\n",
            "============================================================\n",
            "Feature pipeline saved to: accident_severity_model_features\n",
            "Model for Severity 0 saved to: accident_severity_model_severity_0\n",
            "Model for Severity 1 saved to: accident_severity_model_severity_1\n",
            "Model for Severity 2 saved to: accident_severity_model_severity_2\n",
            "Model for Severity 3 saved to: accident_severity_model_severity_3\n",
            "\n",
            "All models saved successfully!\n",
            "   Base path: accident_severity_model\n",
            "\n",
            "======================================================================\n",
            "   EXECUTION COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Total execution time: 271.83 minutes\n",
            "Final F1 Score: 0.6733\n",
            "\n",
            "Key Takeaways:\n",
            "   - One-vs-Rest approach with GBTClassifier (XGBoost-like)\n",
            "   - 4 binary GBTClassifier models trained (one per severity level)\n",
            "   - Class weights applied to each binary model\n",
            "   - Predictions combined using highest probability\n",
            "   - F1 Score is the primary metric for imbalanced classification\n",
            "\n",
            "Spark Session stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3f12405",
        "outputId": "d53592ee-f3a7-400c-a4ed-5d02d4682f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your saved model in Colab\n",
        "model_path_in_colab = '/content/accident_severity_model_severity_0'\n",
        "\n",
        "# Define the destination path in your Google Drive\n",
        "# This will create a folder named 'spark_accident_model' in your Drive's root\n",
        "drive_destination_path = '/content/drive/My Drive/accident_severity_model_severity_3'\n",
        "\n",
        "# Check if the model directory exists in Colab\n",
        "if os.path.exists(model_path_in_colab):\n",
        "    print(f\"Copying model from {model_path_in_colab} to {drive_destination_path}...\")\n",
        "    # Use rsync for more robust copying (creates destination if not exists)\n",
        "    !rsync -avz \"{model_path_in_colab}\" \"{drive_destination_path}\"\n",
        "    print(\"Model copied successfully to Google Drive!\")\n",
        "else:\n",
        "    print(f\"Error: Model directory not found at {model_path_in_colab}. Please ensure the model was saved correctly.\")\n"
      ],
      "metadata": {
        "id": "zBkbV807N3J8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da731c00-e2a3-4a0c-9779-ad74804b680b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Copying model from /content/accident_severity_model_severity_0 to /content/drive/My Drive/accident_severity_model_severity_3...\n",
            "sending incremental file list\n",
            "created directory /content/drive/My Drive/accident_severity_model_severity_3\n",
            "accident_severity_model_severity_0/\n",
            "accident_severity_model_severity_0/data/\n",
            "accident_severity_model_severity_0/data/._SUCCESS.crc\n",
            "accident_severity_model_severity_0/data/.part-00000-0c0df813-ee7d-463d-9a1d-8beaa00240ec-c000.snappy.parquet.crc\n",
            "accident_severity_model_severity_0/data/_SUCCESS\n",
            "accident_severity_model_severity_0/data/part-00000-0c0df813-ee7d-463d-9a1d-8beaa00240ec-c000.snappy.parquet\n",
            "accident_severity_model_severity_0/metadata/\n",
            "accident_severity_model_severity_0/metadata/._SUCCESS.crc\n",
            "accident_severity_model_severity_0/metadata/.part-00000.crc\n",
            "accident_severity_model_severity_0/metadata/_SUCCESS\n",
            "accident_severity_model_severity_0/metadata/part-00000\n",
            "accident_severity_model_severity_0/treesMetadata/\n",
            "accident_severity_model_severity_0/treesMetadata/._SUCCESS.crc\n",
            "accident_severity_model_severity_0/treesMetadata/.part-00000-399cab3d-06e0-453d-bbb1-ae17ddaa9875-c000.snappy.parquet.crc\n",
            "accident_severity_model_severity_0/treesMetadata/_SUCCESS\n",
            "accident_severity_model_severity_0/treesMetadata/part-00000-399cab3d-06e0-453d-bbb1-ae17ddaa9875-c000.snappy.parquet\n",
            "\n",
            "sent 12,713,987 bytes  received 345 bytes  8,476,221.33 bytes/sec\n",
            "total size is 14,651,035  speedup is 1.15\n",
            "Model copied successfully to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tMlRnqvczoAB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}